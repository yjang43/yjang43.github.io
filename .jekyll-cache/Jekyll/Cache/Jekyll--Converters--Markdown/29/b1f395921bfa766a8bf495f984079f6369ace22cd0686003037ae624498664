I"š
<p>To merely consider deep neural network to be a black box puts a constraint of retreiving salient codes.
One of the approaches to disentangle representation for salient codes in generative task is InfoGAN, which is introduced from <a href="https://papers.nips.cc/paper/2016/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Paper.pdf">the paper by Chen et al</a>.
More specifically with examples, salient codes from MNIST data could simply indicate a number itself or others such as width or rotation.</p>

<h3 id="how-does-infogan-work">How Does InfoGAN Work?</h3>
<p>InfoGAN achieves to inject a meaning to representations by maximizing mutual information between latent code of a noise and generated data.
We refer \(c\) to latent codes and \(G(x, c)\) to generated data from noise and the latent code.
Thus, the auxilary term to maximize a mutual information between the two is added to GAN loss,</p>

\[\min_G\max_D V_I(D, G) = V(D, G) - \lambda I(c, G(x, c))\]

<p>, where \(V(D, G)\) indicates GAN loss, \(I(c, G(x, c))\) inidicates mutual information, and \(\lambda\) indicates hyperparmeter to balance the term which in implementation simply set to \(1.0\).</p>

<p>However, maximizing mutual information is a difficult task as it requires a direct access to posterior distribution, \(P(c|x)\).
Thus, auxilary distribution, \(Q(c|x)\), which can be approximated via parametrizing neural network.
Here, author designs a cost efficient network sharing parameters with discriminator.
Below is an overview of the flow of InfoGAN.</p>

<p align="center">
    <img src="/assets/img/infogan_structure.png" alt="cartoon of info gan structure" width="75%" />
    <figcaption style="color: gray; font-size:12px; text-align:center">Network Diagram of InfoGAN</figcaption>
</p>

<h3 id="implementation-in-pytorch">Implementation in PyTorch</h3>

<p>The structure of InfoGAN is quite simple, yet made a significant finding.
Thus, I decided to reimplement the outcome of InfoGAN with PyTorch.
For the concise demonstration of the result, I used one discrete latent code compared to one discrete and two contingous latent codes from the paper to disentangle salient features from MNIST data .</p>

<p>The convoluted training scheme of GAN coming from its mimnimax game made the implementation challeging,
but the result in the end was satisfying.
Below are the images of result from training the model for 50 epochs.</p>

<p align="center">
    <img src="/assets/img/infogan_result.png" alt="cartoon of info gan structure" width="75%" />
    <figcaption style="color: gray; font-size:12px; text-align:center">Grid of randomly generated MNIST dataset with each row corresponds to a fixed discrete latent code [0 </figcaption>
</p>
:ET