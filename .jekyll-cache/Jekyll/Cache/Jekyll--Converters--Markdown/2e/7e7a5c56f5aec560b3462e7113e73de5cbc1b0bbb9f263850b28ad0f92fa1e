I"e<p>We have this simple function of an hyperplane.
\(\mathbf{\omega \cdot x} + b = 0\)
However, getting the optimized hyperplane for SVM
is not so easy as this function looks.</p>

<p>To get an hyperplane that meets the condition,</p>
<div style="width: 100%; overflow: scroll;">
$$y_n \left( \langle \mathbf{\omega , x_n}\rangle  + b \right) \geq 1 $$
</div>
<p>and maximum margin, one can approach by minimizing a loss function of this
operation.</p>

\[min_{w, b} \: {1 \over 2}\|\omega\|^{2} 
+ C \sum max\{0, 1 - y_n \left( \langle \mathbf{\omega , x_n}\rangle  + b \right)\}\]

<p>This can be solved by (sub-) gradient descent methods. A <a href="https://svivek.com/teaching/lectures/slides/svm/svm-sgd.pdf">lecture note from
University of Utah</a>
explains the procedure well. The lecture note suggests stochastic 
gradient descent for the computation speed.</p>
:ET