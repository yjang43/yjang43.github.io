I"	<p>An input embedding layer is most commonly tied with an output embedding layer in the current language model neural network.
This is because Press and Wolf (2017) proved the efficiency of tying weights in those two layers in the paper called,
<em>“Using the Output Embedding to Improve Language Model”</em>.
I always had wondered would there be any trade off by deciding to tie the embeddings, and this paper answered my question by experimentally proving there is no side-effect to worry about.</p>

<h3 id="benefits">Benefits</h3>
<p><strong>The first benefit of weight tying comes in mind is the reduced number of parameters.</strong>
Within a neural networks langauge model (NNLM) Press and Wolf suggest, it shows upto 51% decrease in the number of parameters.
This is the case where three-way weight tying is done in machine translation model,
as same weights are shared across input embedding of encoder, input embedding of decoder, and output embedding of decoder. 
Generally, I believe this much of parameter reduction is not the case for current deeper architectures.
However, as a result of this, it does not only show efficiency in computation of training but also prevents overfitting shown by less perplexity in validation set.</p>

<p align="center">
    <img src="/assets/img/weight_tying.png" alt="weight tying" width="80%" />
</p>

<p><strong>Second benefit, suprisingly, is a better quality of word embeddings.</strong>
Press and Wolf evaluated this quality by making a correlation of human judgement on relation between words to their consine distance of vector representations.
One finding within the evaluation is that a word embedding in NNLM resembles more of an output embedding than of an input embedding when weights are tied.
This implies that an output embedding is actually a better representation of words than an input embedding.
Press and Wolf explain this finding by looking into the gradient of each side of embedding, in which more information is learned in an output embedding throughout training compared to an input embedding</p>

<p align="center">
    <img src="/assets/img/weight_tying_equation.png" alt="gradient flow on input and output embedding" width="50%" />
    <figcaption style="color: gray; font-size:12px; text-align:center">gradient flow on input and output embedding (from the paper)</figcaption>
</p>

<p>Be aware though that weight tying does not apply to word2vec</p>
:ET