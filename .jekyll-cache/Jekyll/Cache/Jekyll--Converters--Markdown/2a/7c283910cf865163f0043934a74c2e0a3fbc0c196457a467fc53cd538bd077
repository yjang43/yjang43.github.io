I"§<p>This is a short summary of a paper, <em>Neural Machine Translation of Rare Words with Subwords Units</em> by <em>Sennrich (2016)</em>.
The approach to formulate a dictionary of corpus is very similar to WordPiece Model proposed by Schuster (2012), which both resolves OOV (out-of-vocabulary) cases by being able to create anywords with subwords.</p>

<h2 id="motivation">Motivation</h2>
<p>Before the dicussion of its algorithm, it will be better to tackle on why new approach to formulating vocabulary is desired.
Original works includes having UNK vocabulary or back-off strategy to cope with OOV cases.
These issues were brought forth due to their fixed vocabulary size.
Using subwords, however, can avoid this because combinations of subwords can create infinite number of words, and to make subwords, we need to segment words into smaller pieces.</p>

<p>However, one question can follow: would  work better?
The answer to that question is simply, yes (in neural translation model specifically).
Languages, not limited only to English, contains such common cases: named entities, cognates and loanwords, morphologically complex words. And, these narrow down to two things to consider: <em>compounding</em> which is to combine words for a word and <em>transliteration</em> which is a translation from letter to letter.</p>

<p>BPE is not a magic that works in any cases unless the balance is found. Size of vocabulary may be small (even down to alphabet level size), and it may still cover limitless words. This of course will be able to maximize time and space efficiency, but with the major cost of delievering information in the sequence.
In other words, small size words will result the same input to be a longer sequence than bigger size words, in which it becomes more dfficult to contain information from its vicinity.</p>

<h2 id="algorithm">Algorithm</h2>
<p>BPE encoding is a compression algorithm that iteratively pairs with the most frequent pairs. In my opinion, this distinct itself from WPM because Schuster (2012) picks pair that optimize language model rather than pairâ€™s frequency.
The code is provided in the paper:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import re, collections

def get_stats<span class="o">(</span>vocab<span class="o">)</span>:
    pairs <span class="o">=</span> collections.defaultdict<span class="o">(</span>int<span class="o">)</span>
    <span class="k">for </span>word, freq <span class="k">in </span>vocab.items<span class="o">()</span>:
        symbols <span class="o">=</span> word.split<span class="o">()</span>
        <span class="k">for </span>i <span class="k">in </span>range<span class="o">(</span>len<span class="o">(</span>symbols<span class="o">)</span><span class="nt">-1</span><span class="o">)</span>:
            pairs[symbols[i], symbols[i+1]] +<span class="o">=</span> freq
    <span class="k">return </span>pairs

def merge_vocab<span class="o">(</span>pair, v_in<span class="o">)</span>:
    v_out <span class="o">=</span> <span class="o">{}</span>
    bigram <span class="o">=</span> re.escape<span class="o">(</span><span class="s1">' '</span>.join<span class="o">(</span>pair<span class="o">))</span>
    p <span class="o">=</span> re.compile<span class="o">(</span>r<span class="s1">'(?&lt;!\S)'</span> + bigram + r<span class="s1">'(?!\S)'</span><span class="o">)</span>
    <span class="k">for </span>word <span class="k">in </span>v_in:
        w_out <span class="o">=</span> p.sub<span class="o">(</span><span class="s1">''</span>.join<span class="o">(</span>pair<span class="o">)</span>, word<span class="o">)</span>
        v_out[w_out] <span class="o">=</span> v_in[word]
    <span class="k">return </span>v_out

vocab <span class="o">=</span> <span class="o">{</span><span class="s1">'l o w &lt;/w&gt;'</span>: 5, <span class="s1">'l o w e r &lt;/w&gt;'</span>: 2,
         <span class="s1">'n e w e s t &lt;/w&gt;'</span>: 6, <span class="s1">'w i d e s t &lt;/w&gt;'</span>: 3<span class="o">}</span>

num_merges <span class="o">=</span> 10
<span class="k">for </span>i <span class="k">in </span>range<span class="o">(</span>num_merges<span class="o">)</span>:
    pairs <span class="o">=</span> get_stats<span class="o">(</span>vocab<span class="o">)</span>
    best <span class="o">=</span> max<span class="o">(</span>pairs, <span class="nv">key</span><span class="o">=</span>pairs.get<span class="o">)</span>
    vocab <span class="o">=</span> merge_vocab<span class="o">(</span>best, vocab<span class="o">)</span>
    print<span class="o">(</span>best<span class="o">)</span>
</code></pre></div></div>

<p>Output is the following when the code is run.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>python bpe_algorithm.py
<span class="o">(</span><span class="s1">'e'</span>, <span class="s1">'s'</span><span class="o">)</span>
<span class="o">(</span><span class="s1">'es'</span>, <span class="s1">'t'</span><span class="o">)</span>
<span class="o">(</span><span class="s1">'est'</span>, <span class="s1">'&lt;/w&gt;'</span><span class="o">)</span>
<span class="o">(</span><span class="s1">'l'</span>, <span class="s1">'o'</span><span class="o">)</span>
<span class="o">(</span><span class="s1">'lo'</span>, <span class="s1">'w'</span><span class="o">)</span>
<span class="o">(</span><span class="s1">'n'</span>, <span class="s1">'e'</span><span class="o">)</span>
<span class="o">(</span><span class="s1">'ne'</span>, <span class="s1">'w'</span><span class="o">)</span>
<span class="o">(</span><span class="s1">'new'</span>, <span class="s1">'est&lt;/w&gt;'</span><span class="o">)</span>
<span class="o">(</span><span class="s1">'low'</span>, <span class="s1">'&lt;/w&gt;'</span><span class="o">)</span>
<span class="o">(</span><span class="s1">'w'</span>, <span class="s1">'i'</span><span class="o">)</span>
</code></pre></div></div>
<p>Note that â€˜estâ€™ which has occurred frequently later combine with â€˜newâ€™ and â€˜lowâ€™ as a whole.
The role of comparison â€˜estâ€™ is cached and combination with other subwords shows a transparent transformation and later help transparent transformation, meaning adjective + â€˜estâ€™ is â€˜ê°€ìž¥â€™ + í˜•ìš©ì‚¬ in Korean.</p>
:ET