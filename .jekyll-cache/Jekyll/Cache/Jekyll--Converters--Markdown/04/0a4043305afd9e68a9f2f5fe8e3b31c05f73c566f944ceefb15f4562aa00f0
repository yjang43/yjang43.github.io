I"Ñ<p>First I approach the idea _distillation from the reading about DistilBERT (Sanh, 2019). 
I decided to learn more about distillation technique, and headed to where the paper directed me: ‚ÄúDistillling the Knowledge in a Neural Network‚Äù (Hinton, 2015).
Distillation is one of trasfer learning techniques to achieve the same or similar performance as big model with a small model.</p>
:ET