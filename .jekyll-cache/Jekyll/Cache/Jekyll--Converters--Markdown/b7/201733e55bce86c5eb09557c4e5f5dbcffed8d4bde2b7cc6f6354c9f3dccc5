I"‡<p>Benepar (Berkley Neural Parser) is a current state-of-the-art constituency parser.
The implementation of the logic is in a paper by Kitaev, ‚ÄúConstituency Parsing with a Self-Attentive Encoder‚Äù.
In a summary, it is a transformer architecture to made decision on the best constituency tree structure each of which are constructed by CKY (Cocke-Kasami-Younger) algorithm.</p>

<h2 id="context-free-grammar">Context Free Grammar</h2>
<p>To understand constituency parsing, knowing CFG (Context Free Grammar) is important.
CFG is a set of grammar rules in which one needs no prior understanding of context of a sentence.
Here are some examples:</p>

<ul>
  <li>S ‚Üí NP VP</li>
  <li>VP ‚Üí Verb NP</li>
</ul>

<p>S, NP, and VP denotes setence, noun phrase, and verb phrase respectively.
There are many more rules.
This means, there are multiple formation of parsed tree given a sentence with many grammar rules.</p>

<p align="center">
    <img src="/assets/img/benepar_example.png" alt="multiple parsed trees" width="50%" />
    <figcaption style="color: gray; font-size:12px; text-align:center">an example where multiple parsed trees exist</figcaption>
</p>

<p>There is a need of scoring system of such that predicts a correct parsed tree, and to do so, neural network comes to the rescue.</p>

<h2 id="model-architecture">Model Architecture</h2>
<p>Kitaev and his colleagues were inspired by previous works of Stern et al (2017) and Gaddy et al (2018).
The model Kitaev proposed is very similar to theirs with the only difference in that contextual span embedding is produced by transformer architecture.</p>

<p>Despite how well transformer architecture works compared to a previous BiLSTM archtiecture, I personally find the way of producing span representation awkward. 
Previsouly in Gaddy et al (2018), span is \(r_{ij} = [f_j - f_i; b_i - b_j]\), which is a concatanation of the difference from point \(i\) to \(j\) of LSTM‚Äôs forward and backward direction.
This formation is very intuitive once understood that forward representation of point \(i\) represents context upto \(i\) and point \(j\) upto \(j\), meaning subtraction of forward pass of \(j\) and \(i\) will result in the portion of representation equivalent to from \(i\) to \(j\).
However, Kitaev mimics the formation of span representation by splitting a token representation in half and pretend each to be forward and backward representation of the token.</p>

<p>Once span representations are produced, score of the span to corresponding label is produced by the following feed forward network:</p>

\[s(i, j, l) = [W_2g(W_1r_{ij} + z_1) + z_2]_l\]

<p>g, z, and W are ReLU activation function, bias, and weight.</p>

<p>By summing span scores, \(s(i, j, l)\), of a tree \(T\), we finally get the score of a tree.</p>

\[s(T) = \sum_{(i,j,l) \in T} s(i,j,l)\]

<p>And at test time or inference time, the optimal parsed tree will be</p>

\[\hat {T} = arg\max_T s(T)\]

<p>And this is trained with hinge loss in the following form:</p>

\[max(0, \max_{T\neq})\]
:ET