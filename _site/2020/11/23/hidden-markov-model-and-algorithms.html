<h3 id="hidden-markov-model">Hidden Markov Model</h3>
<p>HMM is easy to understand knowing its components first.
HMM can represent a sequence of <em>states</em> and <em>symbol</em>.
Symbol is what we observe, 
and state is an information about the symbol.
Simply said, HMM focuses on understanding the meaning of the observed sequence (implication under observation).</p>

<p>If state at location <em>i</em> is denoted as \(\pi_i\).
Probability of a state <em>k</em> to state <em>l</em> is expressed as,</p>

\[a_{kl} = P(\pi=l|\pi=k)\]

<p>This is called <em>transition probability</em> and carries an important role in explaining the first part of HMM, states.
Next, the probability of a symbol in a given state can be denoted as,</p>

<p>\(e_k(b) = P(x_i=b|\pi=k)\)
This is called <em>emission probability</em> and likewise this explains the second part of HMM, symbol.</p>

<h3 id="viterbi">Viterbi</h3>
<p>One question to ask while working with a squential data whose formation is determined by probability is, 
“what is the most probable meaning of the observed sequence?”
This in other words is to ask what the most probable <em>state path</em> is.</p>

\[\tilde{\pi} = argmax_\pi P(x, \pi)\]

<p>Viterbi algorithm provides a solution to this question. The heart of the algorithm is dynamic programming, and the idea comes from the following:</p>

<p><em>If the last state \(\pi_n\) completes the most probable state path, it is completed by connecting to the most probable state path up to \(\pi_{n-1}\)</em></p>

\[v_l(i+1) = e_l(x_{i+1})\max_k(v_k(i)a_{kl})\]

<p>Here <em>i</em>, <em>l</em>, <em>k</em> indicate a position in the squence, a state in question, and a previous state respectively.</p>

<h3 id="forward-algorithm">Forward Algorithm</h3>

<p>Forward algorithm gives a probabilty of a sequence, \(P(x)\) 
simple as that.</p>

\[P(x)=\sum_\pi P(x, \pi)\]

<p>This is a marginalization of states. This can be solved with dynamic programming because \(P(x_1...x_{i}, \pi_i) = \sum_k^l P(x_1...x_i, \pi_i, \pi_{i-1}=k)\).</p>

<p>Thus,</p>

\[when \ f_k(i) = P(x_1...x_i, \pi_i=k)\]

\[f_l(i+1) = e_l(x_{i+1})\sum^l_k f_k(i)a_{kl}\]

<p>The weighted sum of the probabilities of previous sequence from each state is quite intuitive.</p>

