<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-03-23T00:00:27+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">YJ</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">Constituency Parsing with Benepar</title><link href="http://localhost:4000/2021/03/16/benepar.html" rel="alternate" type="text/html" title="Constituency Parsing with Benepar" /><published>2021-03-16T00:00:00+09:00</published><updated>2021-03-16T00:00:00+09:00</updated><id>http://localhost:4000/2021/03/16/benepar</id><content type="html" xml:base="http://localhost:4000/2021/03/16/benepar.html">&lt;p&gt;Benepar (Berkley Neural Parser) is a current state-of-the-art constituency parser.
The implementation of the logic is in a paper by Kitaev and Klein (2018), “Constituency Parsing with a Self-Attentive Encoder”.
In a summary, it is a transformer architecture to made decision on the best constituency tree structure each of which are constructed by CKY (Cocke-Kasami-Younger) algorithm.&lt;/p&gt;

&lt;h2 id=&quot;context-free-grammar&quot;&gt;Context Free Grammar&lt;/h2&gt;
&lt;p&gt;To understand constituency parsing, knowing CFG (Context Free Grammar) is important.
CFG is a set of grammar rules in which one needs no prior understanding of context of a sentence.
Here are some examples:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;S → NP VP&lt;/li&gt;
  &lt;li&gt;VP → Verb NP&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;S, NP, and VP denotes setence, noun phrase, and verb phrase respectively.
There are many more rules.
This means, there are multiple formation of parsed tree given a sentence with many grammar rules.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/benepar_example.png&quot; alt=&quot;multiple parsed trees&quot; width=&quot;50%&quot; /&gt;
    &lt;figcaption style=&quot;color: gray; font-size:12px; text-align:center&quot;&gt;an example where multiple parsed trees exist&lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;There is a need of scoring system of such that predicts a correct parsed tree, and to do so, neural network comes to the rescue.&lt;/p&gt;

&lt;h2 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h2&gt;
&lt;p&gt;Kitaev and Klein were inspired by previous works of Stern et al (2017) and Gaddy et al (2018).
The model Kitaev and Klein proposed is very similar to theirs with the only difference in that contextual span embedding is produced by transformer architecture.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/benepar_architecture.png&quot; alt=&quot;benepar architecture&quot; width=&quot;50%&quot; /&gt;
    &lt;figcaption style=&quot;color: gray; font-size:12px; text-align:center&quot;&gt;benepar architecture&lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;Despite how well transformer architecture works compared to a previous BiLSTM archtiecture, I personally find the way of producing span representation awkward.&lt;/p&gt;

&lt;p&gt;Previsouly in Gaddy et al (2018), span is \(r_{ij} = [f_j - f_i; b_i - b_j]\), which is a concatanation of the difference from point \(i\) to \(j\) of LSTM’s forward and backward direction.
This formation is very intuitive once understood that forward representation of point \(i\) represents context upto \(i\) and point \(j\) upto \(j\), meaning subtraction of forward pass of \(j\) and \(i\) will result in the portion of representation equivalent to from \(i\) to \(j\).&lt;/p&gt;

&lt;p&gt;However, Kitaev and Klein mimic the formation of span representation by splitting a token representation in half and pretend each to be forward and backward representation of the token.&lt;/p&gt;

&lt;p&gt;Once span representations are produced, score of the span to corresponding label is produced by the following feed forward network:&lt;/p&gt;

&lt;div style=&quot;width: 100%; overflow: scroll;&quot;&gt;
$$ s(i, j, l) = [W_2g(W_1r_{ij} + z_1) + z_2]_l $$
&lt;/div&gt;

&lt;p&gt;g, z, and W are ReLU activation function, bias, and weight.&lt;/p&gt;

&lt;p&gt;By summing span scores, \(s(i, j, l)\), of a tree \(T\), we finally get the score of a tree.&lt;/p&gt;

&lt;div style=&quot;width: 100%; overflow: scroll;&quot;&gt;
$$ s(T) = \sum_{(i,j,l) \in T} s(i,j,l)$$
&lt;/div&gt;

&lt;p&gt;Here \((i, j, l) \in T\) can be considered as constituents in a candidate parsed tree from CKY algorithm.&lt;/p&gt;

&lt;p&gt;And at test time or inference time, the optimal parsed tree will be&lt;/p&gt;

\[\hat {T} = arg\max_T s(T)\]

&lt;p&gt;For train time, the model is trained with hinge loss in the following form:&lt;/p&gt;

&lt;div style=&quot;width: 100%; overflow: scroll;&quot;&gt;
$$ max(0, \max_{T\neq T^*}[s(T) + \Delta(T, T^*)] - s(T^*)) \text{ , where } T^* \text{ is the ground truth parsed tree}$$
&lt;/div&gt;

&lt;p&gt;where \(\Delta\) indicates a hamming loss, which is a commonly used to indicate loss in multi label classification task.
This makes sense because multiple labelings are conducted on a tree for each constituent - as VB, NP, ADJP, and etc.
I recommend refering to the paper for more details on the architecture as what drove me to discuss this paper is on the next section.&lt;/p&gt;

&lt;h2 id=&quot;separating-content-and-position-information&quot;&gt;Separating Content and Position Information&lt;/h2&gt;

&lt;p&gt;Kitaev and Klein claim the following: content and position information should be balanced throughout learning, but in practice it does not.
Thus, they seek for a way to separate the information within the network.&lt;/p&gt;

&lt;p&gt;Given the equation \(z_t = w_t + m_t + p_t\), where \(w_t, m_t, p_t\) each means word, tag, and positional embedding,
approach using this was limitted in its peformance as positional information dominates content information.&lt;/p&gt;

&lt;p&gt;Authors’ first attempt was to separate information by concatanating as such: \(z_t = [w_t + m_t;p_t]\).
However, regarding this approach intermingles both information in high deimention network, this method results the same.
For a better explanation, given the following relation \(q = q^{(c)} + q^{(p)}\) and \(k = k^{(c)} + k^{(p)}\),
where \(c\) and \(p\) each means content and position information,
throughout attention mechanism, we see they intermingle and loss balance eventually.&lt;/p&gt;

&lt;div style=&quot;width: 100%; overflow: scroll;&quot;&gt;
    $$ q\cdot k = (q^{(c)} + q^{(p)})\cdot (k^{(c)} + k^{(p)}) $$
&lt;/div&gt;

&lt;p&gt;This happens because weights multiplied to key and value are the &lt;em&gt;weighted sum of both&lt;/em&gt; content and position information.
Thus, authors decide to completely &lt;em&gt;factor&lt;/em&gt; out each inforamtion by reforming weights operated on input like below,&lt;/p&gt;

&lt;div style=&quot;width: 100%; overflow: scroll;&quot;&gt;
    $$ W = \begin{bmatrix} W^{(c)} &amp;amp; 0 \\ 0 &amp;amp; W^{(p)} \end{bmatrix} $$
&lt;/div&gt;

&lt;p&gt;Then, \(W c = [W^{(c)}x^{(c)};W^{(p)}x^{(p)}]\), which means unlike their initial approach,&lt;/p&gt;

&lt;div style=&quot;width: 100%; overflow: scroll;&quot;&gt;
    $$ q\cdot k = q^{(c)}\cdot k^{(c)} + q^{(p)}\cdot k^{(p)}) $$
&lt;/div&gt;</content><author><name></name></author><category term="ml," /><category term="nlp" /><summary type="html">Benepar (Berkley Neural Parser) is a current state-of-the-art constituency parser. The implementation of the logic is in a paper by Kitaev and Klein (2018), “Constituency Parsing with a Self-Attentive Encoder”. In a summary, it is a transformer architecture to made decision on the best constituency tree structure each of which are constructed by CKY (Cocke-Kasami-Younger) algorithm.</summary></entry><entry><title type="html">Distillation in Neural Networks</title><link href="http://localhost:4000/2021/02/09/distilation.html" rel="alternate" type="text/html" title="Distillation in Neural Networks" /><published>2021-02-09T00:00:00+09:00</published><updated>2021-02-09T00:00:00+09:00</updated><id>http://localhost:4000/2021/02/09/distilation</id><content type="html" xml:base="http://localhost:4000/2021/02/09/distilation.html">&lt;p&gt;First I approach the idea distillation from the reading about DistilBERT (Sanh, 2019). 
I decided to learn more about distillation technique, and headed to where the paper directed me: &lt;em&gt;“Distillling the Knowledge in a Neural Network” (Hinton, 2015)&lt;/em&gt;.
Distillation is one of trasfer learning techniques &lt;strong&gt;to achieve the same or similar performance as big model with a small model.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;motif&quot;&gt;Motif&lt;/h2&gt;
&lt;p&gt;At a glance, converting a big model to a small model seems redundant because both models’ performance will be similar (becase that is the goal of distillation – duh!). 
However, distillation provides speed in inference phase, and this is important in real-time applications such as autonomous driving or chatbot customer service.&lt;/p&gt;

&lt;h2 id=&quot;distillation-big-model-to-small-model&quot;&gt;Distillation: Big Model to Small Model&lt;/h2&gt;
&lt;p&gt;Idea seems direct, big model to small model, but steps need a closer look.
Prior to the discussion of nitty gritty of its implementation, intuition of where the knowledge comes from is important.
The authors find this information from soft label, or soft target.
Unlike hard label that deterministically indicates a label of an input, soft label indicates probability, and what the distribution of probability provides is useful information.
For example, if model was to classify dogs from other objects such as cats and ships, it may misclassify dogs to be cats rather than to be ships. Probability distribution contains information that describes such cases.&lt;/p&gt;

&lt;div style=&quot;width: 100%; overflow: scroll;&quot;&gt;
$$ \text{soft label: } \begin{pmatrix}\text{dog: }0.8 &amp;amp; \text{cat: }0.19 &amp;amp; \text{ship: }0.01\end{pmatrix} \\
\text{hard label: } \begin{pmatrix}\text{dog: }1 &amp;amp; \text{cat: }0 &amp;amp; \text{ship: }0\end{pmatrix} $$
&lt;/div&gt;

&lt;p&gt;With this intuition, we naturally move towards softmax to generate the probability distribution. However, softmax alone hardly provide a useable distribution. 
Thus, we need to add and adjust additional term, temperature.
This is where the word, distillation, comes from&lt;/p&gt;

&lt;div style=&quot;width: 100%; overflow: scroll;&quot;&gt;
$$ q_i = {exp(z_i/T)\over \sum_j exp(z_j / T)}\text{, T is temperature} $$
&lt;/div&gt;

&lt;h4 align=&quot;center&quot;&gt;
    &quot;... to raise the temperature of the final softmax until the cumbersome [big] model produces a suitably soft set of targets&quot;.
&lt;/h4&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/distillation.jpg&quot; alt=&quot;distillation&quot; width=&quot;50%&quot; /&gt;
    &lt;figcaption style=&quot;color: gray; font-size:12px; text-align:center&quot;&gt;Distillation is the process of separating components of a mixture based on different boiling points.&lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;The result of increaseing a temperature is explained with the following diagram. 
As temperature rises, more consideration is put on unlikely categories. 
In other words, when model was to predict an input to be a dog, it more acknowledges the similarity between cats and dogs as temperature grows.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/temperature.png&quot; alt=&quot;probability distribution as temperature increases&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;final-loss-function&quot;&gt;Final Loss Function&lt;/h2&gt;
&lt;p&gt;There can be many ways to include distillation to reduce big model to smaller model. The authors suggest to take weighted sum of two loss functions for classficiation task.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Set the same temperature for both big and small models and cross entropy of with soft labels.&lt;/li&gt;
  &lt;li&gt;Like normal classficiation task, cross entropy with hard label.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then, \(T^2\)  should be multiplied to the first loss function as the temperature included in the loss function will decrease the flow of its gradient by \(1 \over T^2\).&lt;/p&gt;</content><author><name></name></author><category term="ml" /><summary type="html">First I approach the idea distillation from the reading about DistilBERT (Sanh, 2019). I decided to learn more about distillation technique, and headed to where the paper directed me: “Distillling the Knowledge in a Neural Network” (Hinton, 2015). Distillation is one of trasfer learning techniques to achieve the same or similar performance as big model with a small model.</summary></entry><entry><title type="html">Byte Pair Encoding in NLP</title><link href="http://localhost:4000/2021/02/03/bpe.html" rel="alternate" type="text/html" title="Byte Pair Encoding in NLP" /><published>2021-02-03T00:00:00+09:00</published><updated>2021-02-03T00:00:00+09:00</updated><id>http://localhost:4000/2021/02/03/bpe</id><content type="html" xml:base="http://localhost:4000/2021/02/03/bpe.html">&lt;p&gt;This is a short summary of a paper, &lt;em&gt;Neural Machine Translation of Rare Words with Subwords Units&lt;/em&gt; by &lt;em&gt;Sennrich (2016)&lt;/em&gt;.
The approach to formulate a dictionary of corpus is very similar to WordPiece Model proposed by Schuster (2012), which both resolves OOV (out-of-vocabulary) cases by being able to create infinite words with subwords.&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Before the dicussion of its algorithm, it will be better to tackle on why new approach of formulating vocabulary is desired.
Original works include having UNK vocabulary or back-off strategy to cope with OOV cases.
These issues were brought forth due to their fixed vocabulary size.
Using subwords, however, can avoid this because combinations of subwords can create infinite number of words, and to make subwords, we need to segment words into smaller pieces.&lt;/p&gt;

&lt;p&gt;However, one question can follow: would it work better?
The answer to that question is simply, yes (in neural translation model specifically).
Languages, not limited only to English, contains such common cases: named entities, cognates and loanwords, morphologically complex words. And, these narrow down to two things to consider: &lt;em&gt;compounding&lt;/em&gt; which is to combine words for a more complex word (air + plane -&amp;gt; airplane) and &lt;em&gt;transliteration&lt;/em&gt; which is a translation from letter to letter (g, b -&amp;gt; ㄱ, ㅂ).&lt;/p&gt;

&lt;p&gt;BPE is not a magic that solves any issues unless the balance is found. Size of vocabulary may be small (even down to alphabet level size), and it will still cover limitless words. This of course will be able to maximize time and space efficiency, but with the major cost of delievering information in the sequence.
In other words, small size words will result the same input to be a longer sequence than bigger size words, in which it becomes more dfficult to contain information from its vicinity.&lt;/p&gt;

&lt;h2 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h2&gt;
&lt;p&gt;BPE encoding is a compression algorithm that iteratively pairs with the most frequent pairs. In my opinion, this distinct itself from WPM because Schuster (2012) picks pair that optimize language model rather than sepcifically for pair’s frequency.
The code is provided in the paper like the following:&lt;/p&gt;

&lt;!-- ``` python --&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;re&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;collections&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pairs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;collections&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;defaultdict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freq&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;pairs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freq&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pairs&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;merge_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;v_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bigram&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;re&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;escape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos; &apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;re&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;(?&amp;lt;!\S)&apos;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigram&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;sa&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;(?!\S)&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;w_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;v_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v_out&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;l o w &amp;lt;/w&amp;gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;l o w e r &amp;lt;/w&amp;gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
         &lt;span class=&quot;s&quot;&gt;&apos;n e w e s t &amp;lt;/w&amp;gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;w i d e s t &amp;lt;/w&amp;gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;num_merges&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_merges&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pairs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;best&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pairs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pairs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;merge_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;!-- ``` --&gt;

&lt;p&gt;When the code is run, we get the ouput like the following.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
(&apos;e&apos;, &apos;s&apos;)&lt;br /&gt;
(&apos;es&apos;, &apos;t&apos;)&lt;br /&gt;
&lt;b&gt;(&apos;est&apos;, &apos;&amp;lt;/w&amp;gt;&apos;)&lt;/b&gt;&lt;br /&gt;
(&apos;l&apos;, &apos;o&apos;)&lt;br /&gt;
(&apos;lo&apos;, &apos;w&apos;)&lt;br /&gt;
(&apos;n&apos;, &apos;e&apos;)&lt;br /&gt;
(&apos;ne&apos;, &apos;w&apos;)&lt;br /&gt;
&lt;b&gt;(&apos;new&apos;, &apos;est&amp;lt;/w&amp;gt;&apos;)&lt;/b&gt;&lt;br /&gt;
(&apos;low&apos;, &apos;&amp;lt;/w&amp;gt;&apos;)&lt;br /&gt;
(&apos;w&apos;, &apos;i&apos;)&lt;br /&gt;
&lt;br /&gt;
&lt;/div&gt;
&lt;p&gt;Note that ‘est’ which has occurred frequently later combine with ‘new’ and ‘low’ as a whole.
The role of comparison ‘est’ is cached and combination with other subwords shows a transparent transformation and later help transparent translation, meaning &lt;em&gt;“adjective + ‘est’“&lt;/em&gt; is &lt;em&gt;&quot;’가장’ + 형용사”&lt;/em&gt; in Korean and parrallel translation can be found within.&lt;/p&gt;</content><author><name></name></author><category term="ml" /><category term="nlp" /><summary type="html">This is a short summary of a paper, Neural Machine Translation of Rare Words with Subwords Units by Sennrich (2016). The approach to formulate a dictionary of corpus is very similar to WordPiece Model proposed by Schuster (2012), which both resolves OOV (out-of-vocabulary) cases by being able to create infinite words with subwords.</summary></entry><entry><title type="html">Autoencoders and Transfer Learning</title><link href="http://localhost:4000/2021/01/22/autoencoders-and-transfer-learning.html" rel="alternate" type="text/html" title="Autoencoders and Transfer Learning" /><published>2021-01-22T00:00:00+09:00</published><updated>2021-01-22T00:00:00+09:00</updated><id>http://localhost:4000/2021/01/22/autoencoders-and-transfer-learning</id><content type="html" xml:base="http://localhost:4000/2021/01/22/autoencoders-and-transfer-learning.html">&lt;h2 id=&quot;autoencoders&quot;&gt;Autoencoders&lt;/h2&gt;
&lt;p&gt;Autoencoders is an unsupervised learning method to reduce a dimensionality of input.
The main idea is very similar to PCA, but more can be done with autoencoder, for example adding non-linearity or regularization. 
The main idea is to reconstruct an input with limitted features.
Here is a diagram of a simple autoencoder.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/autoencoder.png&quot; alt=&quot;JVP&quot; width=&quot;50%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Basically, the network is composed of encoder and decoder, and lantent features (code) gives a high-level representation of input. 
Some tricks on autoencoder can result in better representation of latent features.
One I will mention is denosing autoencoders invented by Vincet(2008). By masking portion of input, it provides robust representation to partially destructed input, which aligns with the definition author proposed to be a “good” representation. Here is a &lt;strong&gt;&lt;a href=&quot;https://github.com/yjang43/ml_practice/blob/master/autoencoder.ipynb&quot;&gt;link&lt;/a&gt;&lt;/strong&gt; that compares the results of autoencoders and desnoising autoencoder.&lt;/p&gt;

&lt;h2 id=&quot;transfer-learning-with-autoencoder&quot;&gt;Transfer Learning with Autoencoder&lt;/h2&gt;
&lt;p&gt;Transfer learning is to use information from source domains, abundant data, to support tasks in target domains, small data.
Glorot(2011) proposes that stacked denoising autoencoder (SDA) can extract intermediate representations from both source and target domain, and use that to help classfication task in target domain.
This idea extends to transfer learning with deep autoencoders (TLDA) by Zhuang (2015), which is a form of supervised learning unlike the usual approach with autoencoder. I have conducted an experiment to check its better performance &lt;strong&gt;&lt;a href=&quot;https://github.com/yjang43/ml_practice/blob/master/TLDA.ipynb&quot;&gt;here&lt;/a&gt;&lt;/strong&gt;.
Instead of using the same dataset from the paper, I used MNIST data.
The task was to check if a binary classfication of a number (ex. is the input ‘1’ when compared with ‘3’) can be transfered to another binary classification (ex. is the input ‘1’ when compared with ‘5’). 
Surprisingly, TLDA performed much better than the baseline, in which I shared parameters trained in simple DNN to another task.
Here is a comparsion of learned feature between TLDA and baseline.
You can see the learned feature from TLDA is more generalized to both the source and target domains.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/tlda_result.png&quot; alt=&quot;JVP&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;</content><author><name></name></author><category term="ml" /><summary type="html">Autoencoders Autoencoders is an unsupervised learning method to reduce a dimensionality of input. The main idea is very similar to PCA, but more can be done with autoencoder, for example adding non-linearity or regularization. The main idea is to reconstruct an input with limitted features. Here is a diagram of a simple autoencoder.</summary></entry><entry><title type="html">Learning Xavier Initialization</title><link href="http://localhost:4000/2021/01/10/xavier-init.html" rel="alternate" type="text/html" title="Learning Xavier Initialization" /><published>2021-01-10T00:00:00+09:00</published><updated>2021-01-10T00:00:00+09:00</updated><id>http://localhost:4000/2021/01/10/xavier-init</id><content type="html" xml:base="http://localhost:4000/2021/01/10/xavier-init.html">&lt;p&gt;Learning PyTorch, I happened to come across a function called &lt;em&gt;reset_parameters()&lt;/em&gt;.
This function included xavier initalization instead of standard initialization.
So, I decided to read Xavier Glorot and Yoshua Bengio’s “Understanding the difficulty of training deep feedforward neural networks”.
The origin paper of Xavier intialization gave me great undersatnding of its objective.&lt;/p&gt;

&lt;h3 id=&quot;xavier-initialization&quot;&gt;Xavier Initialization&lt;/h3&gt;

&lt;p&gt;Objectives of Xavier initialization are simply all about managing variance,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Maintaining variance of forward propagation.&lt;br /&gt;
Each neuron should equally contribute to right prediction.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Maintaining variance of backward propagation&lt;br /&gt;
Back propagation should not be susceptible to vanishing and exploding gradient.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The part that interested me the most is how variance can be calculated and multiplied and added according to the number of layers and hidden neurons within each layer.&lt;/p&gt;

&lt;div style=&quot;width: 100%; overflow: scroll;&quot;&gt;
$$ Var[z^i]  = Var[x]\prod^{i-1}_{i^{&apos;}=0}n_{i^{&apos;}}Var[W^{i^{&apos;}}]$$
&lt;/div&gt;

&lt;p&gt;Moving forward, the authors present the following way to intialize weights from such distribution,&lt;/p&gt;

&lt;div style=&quot;width: 100%; overflow: scroll;&quot;&gt;
$$ W \sim U\Big[-{\sqrt{3}\over{\sqrt{n_{in} + n_{out}}}}, {\sqrt{3}\over{\sqrt{n_{in} + n_{out}}}}\Big] $$
&lt;/div&gt;

&lt;h3 id=&quot;additional-study&quot;&gt;Additional Study&lt;/h3&gt;
&lt;p&gt;Although introduction of Xavier initialization is the main meal of the paper, I enjoyed learning other points made by the authors.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It is important to choose a good activation function.
&lt;br /&gt;
For example, sigmoid shows a quick saturation of hidden layer in the output.
It makes sense because in the early training period, bias overrules features in neurons.
This restricts lower layers to learn meaningful features, and thus slower learning.
Choosing zero-mean activation function line tanh can avoid the problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It is important to choose a right loss function.
&lt;br /&gt;
The authors compared the result between quadratic(LSE) and cross entropy. 
Result shows quadratic loss function gives more plateaus, which I assume it means slower training (flatness gives smaller gradient).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="ml" /><summary type="html">Learning PyTorch, I happened to come across a function called reset_parameters(). This function included xavier initalization instead of standard initialization. So, I decided to read Xavier Glorot and Yoshua Bengio’s “Understanding the difficulty of training deep feedforward neural networks”. The origin paper of Xavier intialization gave me great undersatnding of its objective.</summary></entry><entry><title type="html">Understanding Autograd in PyTorch</title><link href="http://localhost:4000/2020/12/23/pytorch-autograd.html" rel="alternate" type="text/html" title="Understanding Autograd in PyTorch" /><published>2020-12-23T00:00:00+09:00</published><updated>2020-12-23T00:00:00+09:00</updated><id>http://localhost:4000/2020/12/23/pytorch-autograd</id><content type="html" xml:base="http://localhost:4000/2020/12/23/pytorch-autograd.html">&lt;p&gt;I find it very important to understand what autograd is more than simply viewing as a PyTorch engine that computes gradient automatically for us.
Accurately, autograd is an engine for Jacobian-vector product (JVP).
I will cover two things, cycle of its operation and JVP&lt;/p&gt;
&lt;h3 id=&quot;operation-cycle&quot;&gt;Operation Cycle&lt;/h3&gt;
&lt;p&gt;Autograd is possible because operations on Tensors are recorded, and a directed acyclic graph (DAG), or more specifically dynamic computational graph (DCG), is craeted along with it.
In a view of graph, nodes are Tensors, and edges are operations.&lt;/p&gt;

&lt;p&gt;Tensor holds the following important information:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;data&lt;/em&gt;: value a Tensor is holding.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;requires_grad&lt;/em&gt;: &lt;em&gt;VERY IMPORTANT!&lt;/em&gt; tracks operation and forms backward graph which allows backpropagation.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;grad&lt;/em&gt;: stores computed gradient.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;grad_fn&lt;/em&gt;: backward function used to compute gradient.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;is_leaf&lt;/em&gt;: tells if the node is leaf of DCG&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;While the importance of other attributes are self explanatory, it seems odd to store &lt;em&gt;is_leaf&lt;/em&gt;.
However, this becomes an important bit because gradient of Tensor is populated only if &lt;em&gt;requires_grad&lt;/em&gt; and &lt;em&gt;is_leaf&lt;/em&gt; are set to True.&lt;/p&gt;

&lt;h3 id=&quot;jvp&quot;&gt;JVP&lt;/h3&gt;
&lt;p&gt;When I was going through &lt;a href=&quot;https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py&quot;&gt;tutorials&lt;/a&gt; on PyTorch, I found this part very weird and arbitrary.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This explains the case when we call &lt;em&gt;backward()&lt;/em&gt; function from non-scalar output. 
To understand this, I had to learn PyTorch does not (or cannot) compute Jacobian directly to purse simplicity and efficiency.
It uses JVP instead which simply is an inner product of vector and Jacobian.
Normally, the root of DAG, or DCG, of autograd is an output from loss function, simply a scalar value.
This kind of eliminates a reason to compute Jacobian directly and lets us assume computation of gradient always start from a scalar value.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/JVP.png&quot; alt=&quot;JVP&quot; width=&quot;30%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;This does not mean we cannot compute Jacobian.
An example by &lt;a href=&quot;https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments/47026836&quot;&gt;jdhao&lt;/a&gt; gives an intuitive explanation on why and how to directly compute Jacobian.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.autograd&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FloatTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# do backward for first element of z
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FloatTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;retain_graph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#remove gradient in x.grad, or it will be accumulated
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# do backward for second element of z
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FloatTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;retain_graph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# do backward for all elements of z, with weight equal to the derivative of
# loss w.r.t z_1, z_2, z_3 and z_4
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FloatTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;retain_graph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# or we can directly backprop using loss
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# equivalent to loss.backward(torch.FloatTensor([1.0]))
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This gives an output,&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tensor&lt;span class=&quot;o&quot;&gt;([[&lt;/span&gt;2., 0., 0., 0.]]&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
tensor&lt;span class=&quot;o&quot;&gt;([[&lt;/span&gt;0., 2., 0., 0.]]&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
tensor&lt;span class=&quot;o&quot;&gt;([[&lt;/span&gt;2., 2., 2., 2.]]&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
tensor&lt;span class=&quot;o&quot;&gt;([[&lt;/span&gt;2., 2., 2., 2.]]&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This is easy to understand with some math.&lt;/p&gt;

\[\bf x = \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4\end{bmatrix}\]

\[{\bf z} = 2 \odot x\quad (element\ wise)\]

\[{\partial \bf z \over \partial \bf x} =  \begin{bmatrix} {\partial z_1 \over \partial \bf x} &amp;amp; {\partial z_2 \over \partial \bf x} &amp;amp; {\partial z_3 \over \partial \bf x} &amp;amp; {\partial z_4 \over \partial \bf x}\end{bmatrix}\]

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backwardtorch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FloatTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Then, this will mean,&lt;/p&gt;

\[{\partial z_1 \over \partial \bf x} =  \begin{bmatrix} {\partial z_1 \over \partial x_1} &amp;amp; {\partial z_1 \over \partial x_2} &amp;amp; {\partial z_1 \over \partial x_3} &amp;amp; {\partial z_1 \over \partial x_4}\end{bmatrix}\]

&lt;p&gt;Thus, outcome will be,&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; tensor&lt;span class=&quot;o&quot;&gt;([[&lt;/span&gt;2., 0., 0., 0.]]&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;
&lt;p&gt;My confusion on how autograd works was greatly solved by &lt;a href=&quot;https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95&quot;&gt;Vaibhav Kumar’s blog post&lt;/a&gt;. My post is a note on what I find important from it and added an example and mathematic explanation that were not covered in the blog.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/notes/autograd.html&quot;&gt;To learn more about autograd mechanics&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="ml" /><category term="pytorch" /><summary type="html">I find it very important to understand what autograd is more than simply viewing as a PyTorch engine that computes gradient automatically for us. Accurately, autograd is an engine for Jacobian-vector product (JVP). I will cover two things, cycle of its operation and JVP Operation Cycle Autograd is possible because operations on Tensors are recorded, and a directed acyclic graph (DAG), or more specifically dynamic computational graph (DCG), is craeted along with it. In a view of graph, nodes are Tensors, and edges are operations.</summary></entry><entry><title type="html">Attention Is All You Need - Transformer</title><link href="http://localhost:4000/2020/12/03/transformer.html" rel="alternate" type="text/html" title="Attention Is All You Need - Transformer" /><published>2020-12-03T00:00:00+09:00</published><updated>2020-12-03T00:00:00+09:00</updated><id>http://localhost:4000/2020/12/03/transformer</id><content type="html" xml:base="http://localhost:4000/2020/12/03/transformer.html">&lt;h3 id=&quot;thoughts&quot;&gt;Thoughts&lt;/h3&gt;
&lt;p&gt;The paper, “Attention Is All You Need”, is the first to present transformer architecture.
The attention mehanism that this architecture implements is capable of representing dependencies of words without a sequential structure like RNN.
This let the transformer architecture utilize the parallel computation with GPU. This is difficult to acheive in RNN models because of its sequential structure.
The paper states tthe three motifs of self-attention mechanism. One, computational complexity per layer. Two, parallelization. Three, path length for long-range dependencies for inputs. The table below shows the comparison with other networks.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/transformer_table.png&quot; alt=&quot;transformer rnn cnn comparison table&quot; width=&quot;75%&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;architecture&quot;&gt;Architecture&lt;/h3&gt;
&lt;p&gt;I recommend looking back at the article for more details because this part will only stay simple focus more on the overview of the architecture.
Below is the diagram,&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/transformer_architecture.png&quot; alt=&quot;transformer architecture&quot; width=&quot;40%&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;Left part of the diagram is an encoder (multi-head self-attention mechanism + fully connected feed-forward network) and the right is a decoder (multi-head attention over output + multi-head attention connecting encoder and decoder + fully connected feed-forawrd).&lt;/p&gt;
&lt;h4 id=&quot;encoder&quot;&gt;Encoder&lt;/h4&gt;
&lt;p&gt;Starting from the bottom of the encoder, input embedding is done like many other networks into \(d_{model}\) dimension.
Positional encoding is added to the embeddings. From the candidates of positional encodings, one using sine and consine functions is used. I need more background to understand this implementation.
Then, this gets fed into &lt;em&gt;multi-head attention&lt;/em&gt; or scaled dot-product attention layer. This layer is the highlight of the paper, and used in both encoder and encoder and even encoder-decoder in which encoder connects to decoder. 
Difference between multi-head attention and scaled dot-product attention is simply the latter is the single-head version of the former.
We can use parallelization in this step by using multi-head instead.
Here is a diagram of multi-head attention mechansim.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/attention_diagram.png&quot; alt=&quot;attention diagram&quot; width=&quot;75%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;The idea is simply taking a dot product of &lt;em&gt;query&lt;/em&gt; vector and &lt;em&gt;key&lt;/em&gt; vector to decide the weight to put on the &lt;em&gt;value&lt;/em&gt; vector. This shows the depencies of values. 
Then the result gets fed into a feed-forward network with one hidden layer with ReLU activation.&lt;/p&gt;

&lt;h4 id=&quot;decoder&quot;&gt;Decoder&lt;/h4&gt;
&lt;p&gt;Input embedding and positional encoding is identical to an encoder. Also, a feed-forward network after the attention mechanism layer is identical. However, the attention mechanism layers for decoder have some subtle difference. Self-attention layer attends up to the current point which means position after the current point needs to be masked.
This masking stage is shown in the diagram as “Mask (opt.)” and mask is done by putting \(-inf\) to the position after the current point.
Attention layer for connecting encoder and decoder is different to other attention layers in a sense that &lt;em&gt;query&lt;/em&gt; comes from the decoder step and &lt;em&gt;key&lt;/em&gt; and &lt;em&gt;value&lt;/em&gt; comes from the encoder step.&lt;/p&gt;

&lt;p&gt;The overview of transformer ends here. It was a great fun reading the paper. In addition to the link to the paper, I leave a link of YouTube video that explains the structure in much more detail with visuals.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Paper: &lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;https://arxiv.org/pdf/1706.03762.pdf&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;YouTube: &lt;a href=&quot;https://www.youtube.com/watch?v=4Bdc55j80l8&quot;&gt;https://www.youtube.com/watch?v=4Bdc55j80l8&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</content><author><name></name></author><category term="ml" /><category term="nlp" /><category term="transformer" /><summary type="html">Thoughts The paper, “Attention Is All You Need”, is the first to present transformer architecture. The attention mehanism that this architecture implements is capable of representing dependencies of words without a sequential structure like RNN. This let the transformer architecture utilize the parallel computation with GPU. This is difficult to acheive in RNN models because of its sequential structure. The paper states tthe three motifs of self-attention mechanism. One, computational complexity per layer. Two, parallelization. Three, path length for long-range dependencies for inputs. The table below shows the comparison with other networks.</summary></entry><entry><title type="html">Hidden Markov Model and Algorithms - 1</title><link href="http://localhost:4000/2020/11/23/hidden-markov-model-and-algorithms.html" rel="alternate" type="text/html" title="Hidden Markov Model and Algorithms - 1" /><published>2020-11-23T00:00:00+09:00</published><updated>2020-11-23T00:00:00+09:00</updated><id>http://localhost:4000/2020/11/23/hidden-markov-model-and-algorithms</id><content type="html" xml:base="http://localhost:4000/2020/11/23/hidden-markov-model-and-algorithms.html">&lt;h3 id=&quot;hidden-markov-model&quot;&gt;Hidden Markov Model&lt;/h3&gt;
&lt;p&gt;HMM is easy to understand knowing its components first.
HMM can represent a sequence of &lt;em&gt;states&lt;/em&gt; and &lt;em&gt;symbol&lt;/em&gt;.
Symbol is what we observe, 
and state is an information about the symbol.
Simply said, HMM focuses on understanding the meaning of the observed sequence (implication under observation).&lt;/p&gt;

&lt;p&gt;If state at location &lt;em&gt;i&lt;/em&gt; is denoted as \(\pi_i\).
Probability of a state &lt;em&gt;k&lt;/em&gt; to state &lt;em&gt;l&lt;/em&gt; is expressed as,&lt;/p&gt;

&lt;div style=&quot;width: 100%; overflow: scroll;&quot;&gt;
$$a_{kl} = P(\pi=l|\pi=k)$$
&lt;/div&gt;

&lt;p&gt;This is called &lt;em&gt;transition probability&lt;/em&gt; and carries an important role in explaining the first part of HMM, states.
Next, the probability of a symbol in a given state can be denoted as,&lt;/p&gt;

&lt;div style=&quot;width: 100%; overflow: scroll;&quot;&gt;
$$e_k(b) = P(x_i=b|\pi=k)$$
&lt;/div&gt;
&lt;p&gt;This is called &lt;em&gt;emission probability&lt;/em&gt; and likewise this explains the second part of HMM, symbol.&lt;/p&gt;

&lt;h3 id=&quot;viterbi&quot;&gt;Viterbi&lt;/h3&gt;
&lt;p&gt;One question to ask while working with a squential data whose formation is determined by probability is, 
“what is the most probable meaning of the observed sequence?”
This in other words is to ask what the most probable &lt;em&gt;state path&lt;/em&gt; is.&lt;/p&gt;

&lt;div style=&quot;width: 100%; overflow: scroll;&quot;&gt;
$$\tilde{\pi} = argmax_\pi P(x, \pi)$$
&lt;/div&gt;

&lt;p&gt;Viterbi algorithm provides a solution to this question. The heart of the algorithm is dynamic programming, and the idea comes from the following:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If the last state \(\pi_n\) completes the most probable state path, it is completed by connecting to the most probable state path up to \(\pi_{n-1}\)&lt;/em&gt;&lt;/p&gt;

&lt;div style=&quot;width: 100%; overflow: scroll;&quot;&gt;
$$v_l(i+1) = e_l(x_{i+1})\max_k(v_k(i)a_{kl})$$
&lt;/div&gt;

&lt;p&gt;Here &lt;em&gt;i&lt;/em&gt;, &lt;em&gt;l&lt;/em&gt;, &lt;em&gt;k&lt;/em&gt; indicate a position in the squence, a state in question, and a previous state respectively.&lt;/p&gt;

&lt;h3 id=&quot;forward-algorithm&quot;&gt;Forward Algorithm&lt;/h3&gt;

&lt;p&gt;Forward algorithm gives a probabilty of a sequence, \(P(x)\) 
simple as that.&lt;/p&gt;

&lt;div style=&quot;width: 100%; overflow: scroll;&quot;&gt;
$$P(x)=\sum_\pi P(x, \pi)$$
&lt;/div&gt;

&lt;p&gt;This is a marginalization of states. This can be solved with dynamic programming because&lt;/p&gt;
&lt;div style=&quot;width: 100%; overflow: scroll;&quot;&gt;
$$P(x_1...x_{i}, \pi_i) = \sum_k^l P(x_1...x_i, \pi_i, \pi_{i-1}=k)$$
&lt;/div&gt;

&lt;p&gt;Thus,&lt;/p&gt;

&lt;div style=&quot;width: 100%; overflow: scroll;&quot;&gt;
$$when \ f_k(i) = P(x_1...x_i, \pi_i=k)$$
&lt;/div&gt;

&lt;div style=&quot;width: 100%; overflow: scroll;&quot;&gt;
$$f_l(i+1) = e_l(x_{i+1})\sum^l_k f_k(i)a_{kl}$$
&lt;/div&gt;

&lt;p&gt;The weighted sum of the probabilities of previous sequence from each state is quite intuitive.&lt;/p&gt;</content><author><name></name></author><category term="ml" /><category term="hmm" /><summary type="html">Hidden Markov Model HMM is easy to understand knowing its components first. HMM can represent a sequence of states and symbol. Symbol is what we observe, and state is an information about the symbol. Simply said, HMM focuses on understanding the meaning of the observed sequence (implication under observation).</summary></entry><entry><title type="html">Vanilla RNN</title><link href="http://localhost:4000/2020/11/21/vanila-RNN.html" rel="alternate" type="text/html" title="Vanilla RNN" /><published>2020-11-21T00:00:00+09:00</published><updated>2020-11-21T00:00:00+09:00</updated><id>http://localhost:4000/2020/11/21/vanila-RNN</id><content type="html" xml:base="http://localhost:4000/2020/11/21/vanila-RNN.html">&lt;h3 id=&quot;motif&quot;&gt;Motif&lt;/h3&gt;
&lt;p&gt;Language model is the probability of a sequence.
This can be approximated as the product of &lt;em&gt;n&lt;/em&gt;-gram,&lt;/p&gt;

&lt;div style=&quot;width: 100%; overflow: scroll;&quot;&gt;
$$P(w_1,w_2,...,w_m)=\prod^{i=m}_{i=1}P(w_i|w_1,...,w_{i-1})\approx\prod^{i=m}_{i=1}P(w_i|w_1,...,w_{i-1})$$
&lt;/div&gt;

&lt;p&gt;So, &lt;em&gt;n&lt;/em&gt;-gram was a natural approach to do language modeling.
However, this traditional approach has two limitations.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;sparsity of &lt;em&gt;n&lt;/em&gt;-gram&lt;/li&gt;
  &lt;li&gt;exponential growth in model size as &lt;em&gt;n&lt;/em&gt; increases&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;vanila-rnn&quot;&gt;Vanila RNN&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/rnn_structure.png&quot; alt=&quot;RNN structure&quot; width=&quot;40%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;In contrast to the complex network of RNN,
equations to express the network is surprisingly very simple.
This is because weights are repeated used throughout timestamps.&lt;/p&gt;

\[h_t=\sigma(W_hh_{t-1} + W_ex_t)\]

\[\hat{y}=softmax(Uh_t)\]

&lt;p&gt;Here are short description of dimensions of each parameter.&lt;/p&gt;

\[x_t\in\mathbb{R}^d\]

\[W_e\in\mathbb{R}^{D_h\times d}\]

\[W_h\in\mathbb{R}^{D_h\times D_h}\]

\[h_{t-1}\in\mathbb{R}^{D_h}\]

\[\hat{y}\in\mathbb{R}^{|V|}\]

\[U\in\mathbb{R}^{|V| \times D_h}\]

&lt;p&gt;\(d\) indicates dimension of word embeddings &lt;em&gt;(possibly with Word2Vec)&lt;/em&gt;, 
\(D_h\) is a dimension chosen in design. 
\(|V|\) is the cardinality of the corpus.&lt;/p&gt;

&lt;p&gt;Now the structure of RNN is clear, I will discuss the loss and gradient of RNN.
First, loss function looks like this.&lt;/p&gt;

\[J^{(t)}(\Theta) = - \sum^{|V|}_{j=1}y_{t,j}\times log(\hat{y}_{t,j})\]

\[J = {1\over T} \sum^{T}_{t=1}J^{(t)}(\Theta)\]

&lt;p&gt;Here, cross entropy is used to define a distance of proabability.
The loss function of RNN looks a little bit complicated, but it is explained intuitively by looking at the structure of RNN.
\(J^{(t)}\) indicates a loss at a timestamp \(t\). 
Then \(J\) indicates the overall loss of the RNN which is a summation over each timeframe and averaged by dividing the sum by the corpus size.&lt;/p&gt;

&lt;p&gt;Second, gradient looks complicated as well.&lt;/p&gt;

\[{\partial J \over \partial W} = \sum^T_{t=1}{\partial J_t \over \partial W}\]

\[{\partial J_t \over \partial W} = \sum^t_{k=1} {\partial J_t \over \partial y_t}{\partial y_t \over \partial h_t}{\partial h_t \over \partial h_k}{\partial h_k \over \partial W}\]

&lt;p&gt;However, this again is explained intuitively. Similar to how loss at each timestamp is added for an overall loss, overall gradient can decompose into a sum of gradient at each timestamp. Then, gradient at each timestamp is a sum of gradient that imposes change to \(y_t\). This is a multiplication of local gradients in sequence.&lt;/p&gt;

&lt;h3 id=&quot;problem-with-vanila-rnn&quot;&gt;Problem With Vanila RNN&lt;/h3&gt;
&lt;p&gt;With a close look at how gradient is computed in vanila RNN, we find a problem. if elements in each local gradient is bigger than 1, the overall gradient will explode, and vanish otherwise. Each we call &lt;em&gt;exploding gradient&lt;/em&gt; and &lt;em&gt;vanishing gradient&lt;/em&gt; respectively. Thus, variation of RNNs are devised in newer literatures.&lt;/p&gt;</content><author><name></name></author><category term="ml" /><category term="nlp" /><summary type="html">Motif Language model is the probability of a sequence. This can be approximated as the product of n-gram,</summary></entry><entry><title type="html">Bifrost, Possible Solution for DApp</title><link href="http://localhost:4000/2020/06/20/solution-for-dapp.html" rel="alternate" type="text/html" title="Bifrost, Possible Solution for DApp" /><published>2020-06-20T00:00:00+09:00</published><updated>2020-06-20T00:00:00+09:00</updated><id>http://localhost:4000/2020/06/20/solution-for-dapp</id><content type="html" xml:base="http://localhost:4000/2020/06/20/solution-for-dapp.html">&lt;p&gt;The whole reason I came across the idea of DApp is because of the interview
with a company called &lt;em&gt;&lt;a href=&quot;https://pilab.co/&quot;&gt;Pilab&lt;/a&gt;&lt;/em&gt;. This company tries to seek a solution
for a dilemma that DApp’s platform always have: trade-offs in choosing a platform for the service.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://thebifrost.io/static/Bifrost_WP_Eng.pdf&quot;&gt;Bifrost’s white paper&lt;/a&gt;&lt;/em&gt; was interesting reading.
Even if I lacked a DApp background, with a simple understanding of block chain in general, and private and public
block chain, I was able to understand what problem Bifrost intend to solve.&lt;/p&gt;

&lt;p&gt;According to the document, each block chain platform has a different focus as to
its implementation and ecosystem. Some focuses on scalability, and other focuses
on security and beyond.&lt;/p&gt;

&lt;p&gt;As a programmer that needs to choose solely one platform, the risk or loss for
completely giving in one platform is a huge risk. Bifrost comes into place and solve
this dilemma by suggesting a framework, so to speak, that allows running two block
chain platforms in parallel from my understanding. My first concern was decrease in
its efficiency due to running two platforms compared to one, but they are working on
creating its own language called &lt;em&gt;Recipe&lt;/em&gt; that optimizes its performance.&lt;/p&gt;

&lt;p&gt;I am excited to learn more about this technology in the future.&lt;/p&gt;</content><author><name></name></author><category term="blockchain" /><category term="dapp" /><summary type="html">The whole reason I came across the idea of DApp is because of the interview with a company called Pilab. This company tries to seek a solution for a dilemma that DApp’s platform always have: trade-offs in choosing a platform for the service.</summary></entry></feed>