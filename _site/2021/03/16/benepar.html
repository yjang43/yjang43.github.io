<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <title>Constituency Parsing with Benepar</title>

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
    <link rel="stylesheet" href="/assets/css/style.css">
    <link rel="stylesheet" href="/assets/css/paginator.css">
    <link rel="stylesheet" type="text/css" href="/assets/css/syntax.css">
    <!--for kramdown math render-->
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</head>
<body>
    <nav class="navbar navbar-expand-lg navbar-light bg-light">
        <a class="navbar-brand" href="/">YJ</a>
        <ul class="navbar-nav">
            <li class="nav-item">
                <a class="nav-link py-0" href="/projects/">Projects</a>
            </li>
            <li class="nav-item">
                <a class="nav-link py-0" href="/blog/">Blog</a>
            </li>
        </ul>
    </nav>
    <!-- <div class="jumbotron jumbotron-fluid shadow-lg jumbotron-img">
        <div class="container">
        </div>
    </div> -->

    <div class="container-fluid content-pad">

        <h1>Constituency Parsing with Benepar</h1>
        <p>Benepar (Berkley Neural Parser) is a current state-of-the-art constituency parser.
The implementation of the logic is in a paper by Kitaev (2018), “Constituency Parsing with a Self-Attentive Encoder”.
In a summary, it is a transformer architecture to made decision on the best constituency tree structure each of which are constructed by CKY (Cocke-Kasami-Younger) algorithm.</p>

<h2 id="context-free-grammar">Context Free Grammar</h2>
<p>To understand constituency parsing, knowing CFG (Context Free Grammar) is important.
CFG is a set of grammar rules in which one needs no prior understanding of context of a sentence.
Here are some examples:</p>

<ul>
  <li>S → NP VP</li>
  <li>VP → Verb NP</li>
</ul>

<p>S, NP, and VP denotes setence, noun phrase, and verb phrase respectively.
There are many more rules.
This means, there are multiple formation of parsed tree given a sentence with many grammar rules.</p>

<p align="center">
    <img src="/assets/img/benepar_example.png" alt="multiple parsed trees" width="50%" />
    <figcaption style="color: gray; font-size:12px; text-align:center">an example where multiple parsed trees exist</figcaption>
</p>

<p>There is a need of scoring system of such that predicts a correct parsed tree, and to do so, neural network comes to the rescue.</p>

<h2 id="model-architecture">Model Architecture</h2>
<p>Kitaev and his colleagues were inspired by previous works of Stern et al (2017) and Gaddy et al (2018).
The model Kitaev proposed is very similar to theirs with the only difference in that contextual span embedding is produced by transformer architecture.</p>

<p align="center">
    <img src="/assets/img/benepar_architecture.png" alt="benepar architecture" width="50%" />
    <figcaption style="color: gray; font-size:12px; text-align:center">benepar architecture</figcaption>
</p>

<p>Despite how well transformer architecture works compared to a previous BiLSTM archtiecture, I personally find the way of producing span representation awkward.</p>

<p>Previsouly in Gaddy et al (2018), span is \(r_{ij} = [f_j - f_i; b_i - b_j]\), which is a concatanation of the difference from point \(i\) to \(j\) of LSTM’s forward and backward direction.
This formation is very intuitive once understood that forward representation of point \(i\) represents context upto \(i\) and point \(j\) upto \(j\), meaning subtraction of forward pass of \(j\) and \(i\) will result in the portion of representation equivalent to from \(i\) to \(j\).</p>

<p>However, Kitaev mimics the formation of span representation by splitting a token representation in half and pretend each to be forward and backward representation of the token.</p>

<p>Once span representations are produced, score of the span to corresponding label is produced by the following feed forward network:</p>

<div style="width: 100%; overflow: scroll;">
$$ s(i, j, l) = [W_2g(W_1r_{ij} + z_1) + z_2]_l $$
</div>

<p>g, z, and W are ReLU activation function, bias, and weight.</p>

<p>By summing span scores, \(s(i, j, l)\), of a tree \(T\), we finally get the score of a tree.</p>

<div style="width: 100%; overflow: scroll;">
$$ s(T) = \sum_{(i,j,l) \in T} s(i,j,l)$$
</div>

<p>Here \((i, j, l) \in T\) can be considered as constituents in a candidate parsed tree from CKY algorithm.</p>

<p>And at test time or inference time, the optimal parsed tree will be</p>

\[\hat {T} = arg\max_T s(T)\]

<p>For train time, the model is trained with hinge loss in the following form:</p>

<div style="width: 100%; overflow: scroll;">
$$ max(0, \max_{T\neq T^*}[s(T) + \Delta(T, T^*)] - s(T^*)) \text{ , where } T^* \text{ is the ground truth parsed tree}$$
</div>

<p>where \(\Delta\) indicates a hamming loss, which is a commonly used to indicate loss in multi label classification task.
This makes sense because multiple labelings are conducted on a tree for each constituent - as VB, NP, ADJP, and etc.
I recommend refering to the paper for more details on the architecture as what drove me to discuss this paper is on the next section.</p>

<h2 id="separating-content-and-position-information">Separating Content and Position Information</h2>

<p>This part is what motivated me to write a summary on the paper due to its novelty.</p>

<p><strong>TBA</strong></p>


    </div>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>
</html>
