<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <title>Bayesian Probabilities</title>

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
    <link rel="stylesheet" href="/assets/css/style.css">
    <link rel="stylesheet" href="/assets/css/paginator.css">
    <link rel="stylesheet" type="text/css" href="/assets/css/syntax.css">
    <!--for kramdown math render-->
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</head>
<body>
    <nav class="navbar navbar-expand-lg navbar-light bg-light">
        <a class="navbar-brand" href="/">YJ</a>
        <ul class="navbar-nav">
            <li class="nav-item">
                <a class="nav-link py-0" href="/projects/">Projects</a>
            </li>
            <li class="nav-item">
                <a class="nav-link py-0" href="/blog/">Blog</a>
            </li>
        </ul>
    </nav>
    <!-- <div class="jumbotron jumbotron-fluid shadow-lg jumbotron-img">
        <div class="container">
        </div>
    </div> -->

    <div class="container-fluid content-pad">

        <h1>Bayesian Probabilities</h1>
        <p>The two interpretation of probability are frequentist and Bayesian views. 
One good example of frequentist view is training neural network, in which the model is trained minimizing negative log likelihood (maximizing likelihood in other words) observing train data.
Opposite to frequentist view which relies merely on observation of occurrences, Bayesian involves <em>prior probability</em> to prevent some of the shortcomings of frequentist view such as overfitting on small dataset.</p>

<p align="center">
    <img src="/assets/img/bayesian.png" alt="bayesian cartoon" width="75%" />
    <figcaption style="color: gray; font-size:12px; text-align:center"></figcaption>
</p>

<p>Bayesian approach can be further explained from Bayes’ rule.
Bayes’ rule is the following,</p>

<div style="width: 100%; overflow: scroll;">
$$p(D | \mathbf{w}) = {p(D | \mathbf{w}) p(\mathbf{w}) \over p(D)}$$
</div>

<p>\(p(D | \mathbf{w})\) being <em>posterior probability</em>, \(p(D | \mathbf{w})\) being <em>likelihood</em> \(p(\mathbf{w})\) being <em>prior probability</em>.
Thus, the following relation can be drawn,</p>

<div style="width: 100%; overflow: scroll;">
$$\text{posterior} \propto \text{likelihood} \times \text{prior}$$
</div>

<p>This relation provides a profound note that maximizing likelihood function results in maximizing posterior probability.
However, the process of modeling with such process will only work with an appropriate prior probability.
I will further discuss regarding the details of other aspects of Bayesian view in later blog posts.</p>

<hr />
<h2 id="reference">Reference</h2>
<p><a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Bishop, Christopher M. (2006). Pattern recognition and machine learning. New York :Springer,</a></p>


    </div>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>
</html>
