<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <title>Blog</title>

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
    <link rel="stylesheet" href="/assets/css/style.css">
    <link rel="stylesheet" href="/assets/css/paginator.css">
    <link rel="stylesheet" type="text/css" href="/assets/css/syntax.css">
</head>
<body>
    <nav class="navbar navbar-expand-lg navbar-light bg-light">
        <a class="navbar-brand" href="/">YJ</a>
        <ul class="navbar-nav">
            <li class="nav-item">
                <a class="nav-link py-0" href="/projects/">Projects</a>
            </li>
            <li class="nav-item">
                <a class="nav-link py-0" href="/blog/">Blog</a>
            </li>
        </ul>
    </nav>
    <div class="jumbotron jumbotron-fluid shadow-lg jumbotron-img">
        <div class="container">
            <div class="row">
                <h1>Blog</h1>
            </div>
            <div class="row">
                <div class="col-4">
                    <p>Journal during my journey to the programming. Anyone interested in subjects like python, algorithms and AI, you are the first class to this blog. Welcome to my blog!</p>
                </div>
            </div>
        </div>
    </div>

    <!--for kramdown math render-->
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<div class="container-fluid">
    <p class="tag-box">
        <a href="/tags.html">Tags:</a>
        
        jekyll
        
        github
        
        aws
        
        django
        
        python
        
        module
        
        ml
        
        ai
        
        paper
        
        svm
        
        blockchain
        
        dapp
        
        ml,
        
        nlp
        
        hmm
        
        nlp,
        
        transformer
        
        pytorch
        
    </p>
</div>
<div class="container-sm">
    
    <h1>Understanding Autograd in PyTorch</h1>
    23 Dec 2020<br><br>
    <p>I find it very important to understand what autograd is more than simply viewing as a PyTorch engine that computes gradient automatically for us.
Accurately, autograd is an engine for Jacobian-vector product (JVP).
I will cover two things, cycle of its operation and JVP</p>
<h3 id="operation-cycle">Operation Cycle</h3>
<p>Autograd is possible because operations on Tensors are recorded, and a directed acyclic graph (DAG), or more specifically dynamic computational graph (DCG), is craeted along with it.
In a view of graph, nodes are Tensors, and edges are operations.</p>

<p>Tensor holds the following important information:</p>
<ol>
  <li><em>data</em>: value a Tensor is holding.</li>
  <li><em>requires_grad</em>: <em>VERY IMPORTANT!</em> tracks operation and forms backward graph which allows backpropagation.</li>
  <li><em>grad</em>: stores computed gradient.</li>
  <li><em>grad_fn</em>: backward function used to compute gradient.</li>
  <li><em>is_leaf</em>: tells if the node is leaf of DCG</li>
</ol>

<p>While the importance of other attributes are self explanatory, it seems odd to store <em>is_leaf</em>.
However, this becomes an important bit because gradient of Tensor is populated only if <em>requires_grad</em> and <em>is_leaf</em> are set to True.</p>

<h3 id="jvp">JVP</h3>
<p>When I was going through <a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py">tutorials</a> on PyTorch, I found this part very weird and arbitrary.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">y</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>
<p>This explains the case when we call <em>backward()</em> function from non-scalar output. 
To understand this, I had to learn PyTorch does not (or cannot) compute Jacobian directly to purse simplicity and efficiency.
It uses JVP instead which simply is an inner product of vector and Jacobian.
Normally, the root of DAG, or DCG, of autograd is an output from loss function, simply a scalar value.
This kind of eliminates a reason to compute Jacobian directly and lets us assume computation of gradient always start from a scalar value.</p>

<p align="center">
    <img src="/assets/img/JVP.png" alt="JVP" width="30%" />
</p>

<p>This does not mean we cannot compute Jacobian.
An example by <a href="https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments/47026836">jdhao</a> gives an intuitive explanation on why and how to directly compute Jacobian.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># do backward for first element of z
</span><span class="n">z</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span> <span class="c1">#remove gradient in x.grad, or it will be accumulated
</span>
<span class="c1"># do backward for second element of z
</span><span class="n">z</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>

<span class="c1"># do backward for all elements of z, with weight equal to the derivative of
# loss w.r.t z_1, z_2, z_3 and z_4
</span><span class="n">z</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>

<span class="c1"># or we can directly backprop using loss
</span><span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># equivalent to loss.backward(torch.FloatTensor([1.0]))
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>  
</code></pre></div></div>
<p>This gives an output,</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor<span class="o">([[</span>2., 0., 0., 0.]]<span class="o">)</span>
tensor<span class="o">([[</span>0., 2., 0., 0.]]<span class="o">)</span>
tensor<span class="o">([[</span>2., 2., 2., 2.]]<span class="o">)</span>
tensor<span class="o">([[</span>2., 2., 2., 2.]]<span class="o">)</span>
</code></pre></div></div>
<p>This is easy to understand with some math.</p>

\[\bf x = \begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4\end{bmatrix}\]

\[{\bf z} = 2 \odot x\quad (element\ wise)\]

\[{\partial \bf z \over \partial \bf x} =  \begin{bmatrix} {\partial z_1 \over \partial \bf x} &amp; {\partial z_2 \over \partial \bf x} &amp; {\partial z_3 \over \partial \bf x} &amp; {\partial z_4 \over \partial \bf x}\end{bmatrix}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">z</span><span class="p">.</span><span class="n">backwardtorch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>
</code></pre></div></div>
<p>Then, this will mean,</p>

\[{\partial z_1 \over \partial \bf x} =  \begin{bmatrix} {\partial z_1 \over \partial x_1} &amp; {\partial z_1 \over \partial x_2} &amp; {\partial z_1 \over \partial x_3} &amp; {\partial z_1 \over \partial x_4}\end{bmatrix}\]

<p>Thus, outcome will be,</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> tensor<span class="o">([[</span>2., 0., 0., 0.]]<span class="o">)</span>
</code></pre></div></div>

<h3 id="reference">Reference</h3>
<p>My confusion on how autograd works was greatly solved by <a href="https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95">Vaibhav Kumar’s blog post</a>. My post is a note on what I find important from it and added an example and mathematic explanation that were not covered in the blog.</p>

    <div class="post-border"></div>
    
    <h1>Attention Is All You Need - Transformer</h1>
    03 Dec 2020<br><br>
    <h3 id="thoughts">Thoughts</h3>
<p>The paper, “Attention Is All You Need”, is the first to present transformer architecture.
The attention mehanism that this architecture implements is capable of representing dependencies of words without a sequential structure like RNN.
This let the transformer architecture utilize the parallel computation with GPU. This is difficult to acheive in RNN models because of its sequential structure.
The paper states tthe three motifs of self-attention mechanism. One, computational complexity per layer. Two, parallelization. Three, path length for long-range dependencies for inputs. The table below shows the comparison with other networks.</p>

<p align="center">
    <img src="/assets/img/transformer_table.png" alt="transformer rnn cnn comparison table" width="75%" />
</p>

<h3 id="architecture">Architecture</h3>
<p>I recommend looking back at the article for more details because this part will only stay simple focus more on the overview of the architecture.
Below is the diagram,</p>
<p align="center">
    <img src="/assets/img/transformer_architecture.png" alt="transformer architecture" width="40%" />
</p>
<p>Left part of the diagram is an encoder (multi-head self-attention mechanism + fully connected feed-forward network) and the right is a decoder (multi-head attention over output + multi-head attention connecting encoder and decoder + fully connected feed-forawrd).</p>
<h4 id="encoder">Encoder</h4>
<p>Starting from the bottom of the encoder, input embedding is done like many other networks into \(d_{model}\) dimension.
Positional encoding is added to the embeddings. From the candidates of positional encodings, one using sine and consine functions is used. I need more background to understand this implementation.
Then, this gets fed into <em>multi-head attention</em> or scaled dot-product attention layer. This layer is the highlight of the paper, and used in both encoder and encoder and even encoder-decoder in which encoder connects to decoder. 
Difference between multi-head attention and scaled dot-product attention is simply the latter is the single-head version of the former.
We can use parallelization in this step by using multi-head instead.
Here is a diagram of multi-head attention mechansim.</p>

<p align="center">
    <img src="/assets/img/attention_diagram.png" alt="attention diagram" width="75%" />
</p>

<p>The idea is simply taking a dot product of <em>query</em> vector and <em>key</em> vector to decide the weight to put on the <em>value</em> vector. This shows the depencies of values. 
Then the result gets fed into a feed-forward network with one hidden layer with ReLU activation.</p>

<h4 id="decoder">Decoder</h4>
<p>Input embedding and positional encoding is identical to an encoder. Also, a feed-forward network after the attention mechanism layer is identical. However, the attention mechanism layers for decoder have some subtle difference. Self-attention layer attends up to the current point which means position after the current point needs to be masked.
This masking stage is shown in the diagram as “Mask (opt.)” and mask is done by putting \(-inf\) to the position after the current point.
Attention layer for connecting encoder and decoder is different to other attention layers in a sense that <em>query</em> comes from the decoder step and <em>key</em> and <em>value</em> comes from the encoder step.</p>

<p>The overview of transformer ends here. It was a great fun reading the paper. In addition to the link to the paper, I leave a link of YouTube video that explains the structure in much more detail with visuals.</p>

<p><em>Paper: <a href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a></em></p>

<p><em>YouTube: <a href="https://www.youtube.com/watch?v=4Bdc55j80l8">https://www.youtube.com/watch?v=4Bdc55j80l8</a></em></p>

    <div class="post-border"></div>
    
    <h1>Hidden Markov Model and Algorithms - 1</h1>
    23 Nov 2020<br><br>
    <h3 id="hidden-markov-model">Hidden Markov Model</h3>
<p>HMM is easy to understand knowing its components first.
HMM can represent a sequence of <em>states</em> and <em>symbol</em>.
Symbol is what we observe, 
and state is an information about the symbol.
Simply said, HMM focuses on understanding the meaning of the observed sequence (implication under observation).</p>

<p>If state at location <em>i</em> is denoted as \(\pi_i\).
Probability of a state <em>k</em> to state <em>l</em> is expressed as,</p>

\[a_{kl} = P(\pi=l|\pi=k)\]

<p>This is called <em>transition probability</em> and carries an important role in explaining the first part of HMM, states.
Next, the probability of a symbol in a given state can be denoted as,</p>

<p>\(e_k(b) = P(x_i=b|\pi=k)\)
This is called <em>emission probability</em> and likewise this explains the second part of HMM, symbol.</p>

<h3 id="viterbi">Viterbi</h3>
<p>One question to ask while working with a squential data whose formation is determined by probability is, 
“what is the most probable meaning of the observed sequence?”
This in other words is to ask what the most probable <em>state path</em> is.</p>

\[\tilde{\pi} = argmax_\pi P(x, \pi)\]

<p>Viterbi algorithm provides a solution to this question. The heart of the algorithm is dynamic programming, and the idea comes from the following:</p>

<p><em>If the last state \(\pi_n\) completes the most probable state path, it is completed by connecting to the most probable state path up to \(\pi_{n-1}\)</em></p>

\[v_l(i+1) = e_l(x_{i+1})\max_k(v_k(i)a_{kl})\]

<p>Here <em>i</em>, <em>l</em>, <em>k</em> indicate a position in the squence, a state in question, and a previous state respectively.</p>

<h3 id="forward-algorithm">Forward Algorithm</h3>

<p>Forward algorithm gives a probabilty of a sequence, \(P(x)\) 
simple as that.</p>

\[P(x)=\sum_\pi P(x, \pi)\]

<p>This is a marginalization of states. This can be solved with dynamic programming because \(P(x_1...x_{i}, \pi_i) = \sum_k^l P(x_1...x_i, \pi_i, \pi_{i-1}=k)\).</p>

<p>Thus,</p>

\[when \ f_k(i) = P(x_1...x_i, \pi_i=k)\]

\[f_l(i+1) = e_l(x_{i+1})\sum^l_k f_k(i)a_{kl}\]

<p>The weighted sum of the probabilities of previous sequence from each state is quite intuitive.</p>


    <div class="post-border"></div>
    
    <h1>Vanilla RNN</h1>
    21 Nov 2020<br><br>
    <h3 id="motif">Motif</h3>
<p>Language model is the probability of a sequence.
This can be approximated as the product of <em>n</em>-gram,</p>

\[P(w_1,w_2,...,w_m)=\prod^{i=m}_{i=1}P(w_i|w_1,...,w_{i-1})\approx\prod^{i=m}_{i=1}P(w_i|w_1,...,w_{i-1})\]

<p>So, <em>n</em>-gram was a natural approach to do language modeling.
However, this traditional approach has two limitations.</p>
<ul>
  <li>sparsity of <em>n</em>-gram</li>
  <li>exponential growth in model size as <em>n</em> increases</li>
</ul>

<h3 id="vanila-rnn">Vanila RNN</h3>
<p align="center">
    <img src="/assets/img/rnn_structure.png" alt="RNN structure" width="40%" />
</p>

<p>In contrast to the complex network of RNN,
equations to express the network is surprisingly very simple.
This is because weights are repeated used throughout timestamps.</p>

\[h_t=\sigma(W_hh_{t-1} + W_ex_t)\]

\[\hat{y}=softmax(Uh_t)\]

<p>Here are short description of dimensions of each parameter.</p>

\[x_t\in\mathbb{R}^d\]

\[W_e\in\mathbb{R}^{D_h\times d}\]

\[W_h\in\mathbb{R}^{D_h\times D_h}\]

\[h_{t-1}\in\mathbb{R}^{D_h}\]

\[\hat{y}\in\mathbb{R}^{|V|}\]

\[U\in\mathbb{R}^{|V| \times D_h}\]

<p>\(d\) indicates dimension of word embeddings <em>(possibly with Word2Vec)</em>, 
\(D_h\) is a dimension chosen in design. 
\(|V|\) is the cardinality of the corpus.</p>

<p>Now the structure of RNN is clear, I will discuss the loss and gradient of RNN.
First, loss function looks like this.</p>

\[J^{(t)}(\Theta) = - \sum^{|V|}_{j=1}y_{t,j}\times log(\hat{y}_{t,j})\]

\[J = {1\over T} \sum^{T}_{t=1}J^{(t)}(\Theta)\]

<p>Here, cross entropy is used to define a distance of proabability.
The loss function of RNN looks a little bit complicated, but it is explained intuitively by looking at the structure of RNN.
\(J^{(t)}\) indicates a loss at a timestamp \(t\). 
Then \(J\) indicates the overall loss of the RNN which is a summation over each timeframe and averaged by dividing the sum by the corpus size.</p>

<p>Second, gradient looks complicated as well.</p>

\[{\partial J \over \partial W} = \sum^T_{t=1}{\partial J_t \over \partial W}\]

\[{\partial J_t \over \partial W} = \sum^t_{k=1} {\partial J_t \over \partial y_t}{\partial y_t \over \partial h_t}{\partial h_t \over \partial h_k}{\partial h_k \over \partial W}\]

<p>However, this again is explained intuitively. Similar to how loss at each timestamp is added for an overall loss, overall gradient can decompose into a sum of gradient at each timestamp. Then, gradient at each timestamp is a sum of gradient that imposes change to \(y_t\). This is a multiplication of local gradients in sequence.</p>

<h3 id="problem-with-vanila-rnn">Problem With Vanila RNN</h3>
<p>With a close look at how gradient is computed in vanila RNN, we find a problem. if elements in each local gradient is bigger than 1, the overall gradient will explode, and vanish otherwise. Each we call <em>exploding gradient</em> and <em>vanishing gradient</em> respectively. Thus, variation of RNNs are devised in newer literatures.</p>

    <div class="post-border"></div>
    
</div>

<div class="pagination">
  
    <span class=page_num>
      <span>&laquo; Prev</span>
    </span>
  

  
    
      <span class=page_num>
        <em>1</em>
      </span>
    
  
    
      <span class=page_num>
        <a href="/blog/page2/">2 </a>
      </span>
    
  
    
      <span class=page_num>
        <a href="/blog/page3/">3 </a>
      </span>
    
  

  
    <span class=page_num>
      <a href="/blog/page2/">Next &raquo;</a>
    </span>
  
</div>



    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>
</body>
</html>
