<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <title>Blog</title>

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
    <link rel="stylesheet" href="/assets/css/style.css">
    <link rel="stylesheet" type="text/css" href="/assets/css/syntax.css">
</head>
<body>
    <nav class="navbar navbar-expand-lg navbar-light bg-light">
        <a class="navbar-brand" href="/">YJ</a>
        <ul class="navbar-nav">
            <li class="nav-item">
                <a class="nav-link py-0" href="/projects/">Projects</a>
            </li>
            <li class="nav-item">
                <a class="nav-link py-0" href="/blog/">Blog</a>
            </li>
        </ul>
    </nav>
    <div class="jumbotron jumbotron-fluid shadow-lg jumbotron-img">
        <div class="container">
            <div class="row">
                <h1>Blog</h1>
            </div>
            <div class="row">
                <div class="col-4">
                    <p>Journal during my journey to the programming. Anyone interested in subjects like python, algorithms and AI, you are the first class to this blog. Welcome to my blog!</p>
                </div>
            </div>
        </div>
    </div>

    <!--for kramdown math render-->
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<div class="container-fluid">
    <p class="tag-box">
        <a href="/tags.html">Tags:</a>
        
        jekyll
        
        github
        
        aws
        
        django
        
        python
        
        module
        
        ml
        
        ai
        
        paper
        
        svm
        
        blockchain
        
        dapp
        
        ml,
        
        nlp
        
        hmm
        
    </p>
</div>
<div class="container-sm">
    
    <h1>Hidden Markov Model and Algorithms</h1>
    23 Nov 2020<br><br>
    <h3 id="hidden-markov-model">Hidden Markov Model</h3>
<p>HMM is easy to understand knowing its components first.
HMM can represent a sequence of <em>states</em> and <em>symbol</em>.
Symbol is what we observe, 
and state is an information about the symbol.
Simply said, HMM focuses on understanding the meaning of the observed sequence (implication under observation).</p>

<p>If state at location <em>i</em> is denoted as \(\pi_i\).
Probability of a state <em>k</em> to state <em>l</em> is expressed as,</p>

\[a_{kl} = P(\pi=l|\pi=k)\]

<p>This is called <em>transition probability</em> and carries an important role in explaining the first part of HMM, states.
Next, the probability of a symbol in a given state can be denoted as,</p>

<p>\(e_k(b) = P(x_i=b|\pi=k)\)
This is called <em>emission probability</em> and likewise this explains the second part of HMM, symbol.</p>

<h3 id="viterbi">Viterbi</h3>
<p>One question to ask while working with a squential data whose formation is determined by probability is, 
“what is the most probable meaning of the observed sequence?”
This in other words is to ask what the most probable <em>state path</em> is.</p>

\[\tilde{\pi} = argmax_\pi P(x, \pi)\]

<p>Viterbi algorithm provides a solution to this question. The heart of the algorithm is dynamic programming, and the idea comes from the following:</p>

<p><em>If the last state \(\pi_n\) completes the most probable state path, it is completed by connecting to the most probable state path up to \(\pi_{n-1}\)</em></p>

\[v_l(i+1) = e_l(x_{i+1})\max_k(v_k(i)a_{kl})\]

<p>Here <em>i</em>, <em>l</em>, <em>k</em> indicate a position in the squence, a state in question, and a previous state respectively.</p>

<h3 id="forward-algorithm">Forward Algorithm</h3>

<p>Forward algorithm gives a probabilty of a sequence, \(P(x)\) 
simple as that.</p>

\[P(x)=\sum_\pi P(x, \pi)\]

<p>This is a marginalization of states. This can be solved with dynamic programming because \(P(x_1...x_{i}, \pi_i) = \sum_k^l P(x_1...x_i, \pi_i, \pi_{i-1}=k)\).</p>

<p>Thus,</p>

\[when \ f_k(i) = P(x_1...x_i, \pi_i=k)\]

\[f_l(i+1) = e_l(x_{i+1})\sum^l_k f_k(i)a_{kl}\]

<p>The weighted sum of the probabilities of previous sequence from each state is quite intuitive.</p>


    <div class="post-border"></div>
    
    <h1>Vanilla RNN</h1>
    21 Nov 2020<br><br>
    <h3 id="motif">Motif</h3>
<p>Language model is the probability of a sequence.
This can be approximated as the product of <em>n</em>-gram,</p>

\[P(w_1,w_2,...,w_m)=\prod^{i=m}_{i=1}P(w_i|w_1,...,w_{i-1})\approx\prod^{i=m}_{i=1}P(w_i|w_1,...,w_{i-1})\]

<p>So, <em>n</em>-gram was a natural approach to do language modeling.
However, this traditional approach has two limitations.</p>
<ul>
  <li>sparsity of <em>n</em>-gram</li>
  <li>exponential growth in model size as <em>n</em> increases</li>
</ul>

<h3 id="vanila-rnn">Vanila RNN</h3>
<p><img src="/assets/img/rnn_structure.png" alt="RNN structure" align="center" width="450" /></p>

<p>In contrast to the complex network of RNN,
equations to express the network is surprisingly very simple.
This is because weights are repeated used throughout timestamps.</p>

\[h_t=\sigma(W_hh_{t-1} + W_ex_t)\]

\[\hat{y}=softmax(Uh_t)\]

<p>Here are short description of dimensions of each parameter.</p>

\[x_t\in\mathbb{R}^d\]

\[W_e\in\mathbb{R}^{D_h\times d}\]

\[W_h\in\mathbb{R}^{D_h\times D_h}\]

\[h_{t-1}\in\mathbb{R}^{D_h}\]

\[\hat{y}\in\mathbb{R}^{|V|}\]

\[U\in\mathbb{R}^{|V| \times D_h}\]

<p>\(d\) indicates dimension of word embeddings <em>(possibly with Word2Vec)</em>, 
\(D_h\) is a dimension chosen in design. 
\(|V|\) is the cardinality of the corpus.</p>

<p>Now the structure of RNN is clear, I will discuss the loss and gradient of RNN.
First, loss function looks like this.</p>

\[J^{(t)}(\Theta) = - \sum^{|V|}_{j=1}y_{t,j}\times log(\hat{y}_{t,j})\]

\[J = {1\over T} \sum^{T}_{t=1}J^{(t)}(\Theta)\]

<p>Here, cross entropy is used to define a distance of proabability.
The loss function of RNN looks a little bit complicated, but it is explained intuitively by looking at the structure of RNN.
\(J^{(t)}\) indicates a loss at a timestamp \(t\). 
Then \(J\) indicates the overall loss of the RNN which is a summation over each timeframe and averaged by dividing the sum by the corpus size.</p>

<p>Second, gradient looks complicated as well.</p>

\[{\partial J \over \partial W} = \sum^T_{t=1}{\partial J_t \over \partial W}\]

\[{\partial J_t \over \partial W} = \sum^t_{k=1} {\partial J_t \over \partial y_t}{\partial y_t \over \partial h_t}{\partial h_t \over \partial h_k}{\partial h_k \over \partial W}\]

<p>However, this again is explained intuitively. Similar to how loss at each timestamp is added for an overall loss, overall gradient can decompose into a sum of gradient at each timestamp. Then, gradient at each timestamp is a sum of gradient that imposes change to \(y_t\). This is a multiplication of local gradients in sequence.</p>

<h3 id="problem-with-vanila-rnn">Problem With Vanila RNN</h3>
<p>With a close look at how gradient is computed in vanila RNN, we find a problem. if elements in each local gradient is bigger than 1, the overall gradient will explode, and vanish otherwise. Each we call <em>exploding gradient</em> and <em>vanishing gradient</em> respectively. Thus, variation of RNNs are devised in newer literatures.</p>

    <div class="post-border"></div>
    
    <h1>Bifrost, Possible Solution for DApp</h1>
    20 Jun 2020<br><br>
    <p>The whole reason I came across the idea of DApp is because of the interview
with a company called <em><a href="https://pilab.co/">Pilab</a></em>. This company tries to seek a solution
for a dilemma that DApp’s platform always have: trade-offs in choosing a platform for the service.</p>

<p><em><a href="https://thebifrost.io/static/Bifrost_WP_Eng.pdf">Bifrost’s white paper</a></em> was interesting reading.
Even if I lacked a DApp background, with a simple understanding of block chain in general, and private and public
block chain, I was able to understand what problem Bifrost intend to solve.</p>

<p>According to the document, each block chain platform has a different focus as to
its implementation and ecosystem. Some focuses on scalability, and other focuses
on security and beyond.</p>

<p>As a programmer that needs to choose solely one platform, the risk or loss for
completely giving in one platform is a huge risk. Bifrost comes into place and solve
this dilemma by suggesting a framework, so to speak, that allows running two block
chain platforms in parallel from my understanding. My first concern was decrease in
its efficiency due to running two platforms compared to one, but they are working on
creating its own language called <em>Recipe</em> that optimizes its performance.</p>

<p>I am excited to learn more about this technology in the future.</p>

    <div class="post-border"></div>
    
    <h1>Training Hyperplane for Support Vector Machine</h1>
    19 Jun 2020<br><br>
    <p>We have this simple function of an hyperplane.
\(\mathbf{\omega \cdot x} + b = 0\)
However, getting the optimized hyperplane for SVM
is not so easy as this function looks.</p>

<p>To get an hyperplane that meets the condition,
\(y_n \left( \langle \mathbf{\omega , x_n}\rangle  + b \right) \geq 1\)
and maximum margin, one can approach by minimizing a loss function of this
operation.</p>

\[min_{w, b} \: {1 \over 2}\|\omega\|^{2} 
+ C \sum max\{0, 1 - y_n \left( \langle \mathbf{\omega , x_n}\rangle  + b \right)\}\]

<p>This can be solved by (sub-) gradient descent methods. A <a href="https://svivek.com/teaching/lectures/slides/svm/svm-sgd.pdf">lecture note from
University of Utah</a>
explains the procedure well. The lecture note suggests stochastic 
gradient descent for the computation speed.</p>

    <div class="post-border"></div>
    
</div>

<div class="pagination">
  
    <span>&laquo; Prev</span>
  

  
    
      <em>1</em>
    
  
    
      <a href="/blog/page2/">2 </a>
    
  
    
      <a href="/blog/page3/">3 </a>
    
  

  
    <a href="/blog/page2/">Next &raquo;</a>
  
</div>



    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>
</body>
</html>
