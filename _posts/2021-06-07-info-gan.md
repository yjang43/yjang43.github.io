---
title: Disentagle Representation with InfoGAN
tags: ml, gan
img_url: /assets/img/info_gan_face.png
layout: blog
---

To merely consider deep neural network to be a black box puts a constraint of retreiving salient codes.
One of the approaches to disentangle representation for salient codes in generative task is InfoGAN, which is introduced from [the paper by Chen et al](https://papers.nips.cc/paper/2016/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Paper.pdf).
More specifically with examples, salient codes from MNIST data could simply indicate a formulation of number itself or others such as width or rotation.

### How Does InfoGAN Work?
InfoGAN achieves to inject a meaning to representations by maximizing mutual information between latent code of a noise and generated data.
We refer latent codes to $$c$$ and generated data from noise and the latent code to $$G(x, c)$$.
Thus, the auxilary term to maximize a mutual information between the two is added to GAN loss,

$$ \min_G\max_D V_I(D, G) = V(D, G) - \lambda I(c, G(x, c))$$

, where $$V(D, G)$$ indicates GAN loss, $$I(c, G(x, c))$$ inidicates mutual information, and $$\lambda$$ indicates hyperparmeter to balance the term which in implementation simply set to $$1.0$$.

However, maximizing mutual information is a difficult task as it requires a direct access to posterior distribution, $$P(c|x)$$.
Thus, auxilary distribution, $$Q(c|x)$$, which can be approximated via parametrizing neural network, is used instead.
Here, author designs a cost efficient network sharing parameters with discriminator to approximate $$Q$$.
Below is an overview of the flow of InfoGAN and demonstration of how weights are shared.

<p align="center">
    <img src="/assets/img/infogan_structure.png" alt="cartoon of info gan structure"  width="75%"/>
    <figcaption style="color: gray; font-size:12px; text-align:center">Network Diagram of InfoGAN</figcaption>
</p>

### Implementation in PyTorch

The structure of InfoGAN is quite simple, yet made a significant finding.
Thus, I decided to reimplement the outcome of InfoGAN with PyTorch.
Code implementation can be found in my GitHub page __[yjang43/InfoGAN](https://github.com/yjang43/InfoGAN)__.
For the concise demonstration of the result, I used only one discrete latent code as opposed to one discrete and two contingous latent codes from the paper to disentangle salient features from MNIST data.
Below is the sample of generated MNIST data.
<p align="center">
    <img src="/assets/img/infogan_result.png" alt="InfoGAN generated MNIST"  width="75%"/>
    <figcaption style="color: gray; font-size:12px; text-align:center">Grid of randomly generated MNIST dataset with each row corresponding to a fixed discrete latent code [0 ~ 9]</figcaption>
</p>
The convoluted training scheme of GAN coming from its mimnimax game made the implementation challeging,
but the result in the end was satisfying.
Below graph is a result of training loss for 50 epochs with batch size of 16.

<p align="center">
    <img src="/assets/img/infogan_train_loss.png" alt="InfoGAN train loss for 50 epochs"  width="50%"/>
    <figcaption style="color: gray; font-size:12px; text-align:center">Progresion of train loss through 50 epochs</figcaption>
</p>



