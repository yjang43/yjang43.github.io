---
title: Constituency Parsing with Benepar
tags: ml, nlp
img_url: /assets/img/benepar.png
layout: blog
---

Benepar (Berkley Neural Parser) is a current state-of-the-art constituency parser.
The implementation of the logic is in a paper by Kitaev and Klein (2018), "Constituency Parsing with a Self-Attentive Encoder".
In a summary, it is a transformer architecture to made decision on the best constituency tree structure each of which are constructed by CKY (Cocke-Kasami-Younger) algorithm.

## Context Free Grammar
To understand constituency parsing, knowing CFG (Context Free Grammar) is important.
CFG is a set of grammar rules in which one needs no prior understanding of context of a sentence.
Here are some examples:

* S &#8594; NP VP
* VP &#8594; Verb NP

S, NP, and VP denotes setence, noun phrase, and verb phrase respectively.
There are many more rules.
This means, there are multiple formation of parsed tree given a sentence with many grammar rules. 

<p align="center">
    <img src="/assets/img/benepar_example.png" alt="multiple parsed trees"  width="50%"/>
    <figcaption style="color: gray; font-size:12px; text-align:center">an example where multiple parsed trees exist</figcaption>
</p>

There is a need of scoring system of such that predicts a correct parsed tree, and to do so, neural network comes to the rescue.


## Model Architecture
Kitaev and Klein were inspired by previous works of Stern et al (2017) and Gaddy et al (2018).
The model Kitaev and Klein proposed is very similar to theirs with the only difference in that contextual span embedding is produced by transformer architecture. 

<p align="center">
    <img src="/assets/img/benepar_architecture.png" alt="benepar architecture"  width="50%"/>
    <figcaption style="color: gray; font-size:12px; text-align:center">benepar architecture</figcaption>
</p>

Despite how well transformer architecture works compared to a previous BiLSTM archtiecture, I personally find the way of producing span representation awkward. 

Previsouly in Gaddy et al (2018), span is $$r_{ij} = [f_j - f_i; b_i - b_j]$$, which is a concatanation of the difference from point $$i$$ to $$j$$ of LSTM's forward and backward direction.
This formation is very intuitive once understood that forward representation of point $$ i $$ represents context upto $$ i $$ and point $$ j $$ upto $$ j $$, meaning subtraction of forward pass of $$ j $$ and $$ i $$ will result in the portion of representation equivalent to from $$i$$ to $$j$$.

However, Kitaev and Klein mimic the formation of span representation by splitting a token representation in half and pretend each to be forward and backward representation of the token.

Once span representations are produced, score of the span to corresponding label is produced by the following feed forward network:

<div style="width: 100%; overflow: scroll;">
$$ s(i, j, l) = [W_2g(W_1r_{ij} + z_1) + z_2]_l $$
</div>

g, z, and W are ReLU activation function, bias, and weight.

By summing span scores, $$ s(i, j, l) $$, of a tree $$ T $$, we finally get the score of a tree.

<div style="width: 100%; overflow: scroll;">
$$ s(T) = \sum_{(i,j,l) \in T} s(i,j,l)$$
</div>

Here $$ (i, j, l) \in T $$ can be considered as constituents in a candidate parsed tree from CKY algorithm.

And at test time or inference time, the optimal parsed tree will be

$$ \hat {T} = arg\max_T s(T) $$

For train time, the model is trained with hinge loss in the following form:

<div style="width: 100%; overflow: scroll;">
$$ max(0, \max_{T\neq T^*}[s(T) + \Delta(T, T^*)] - s(T^*)) \text{ , where } T^* \text{ is the ground truth parsed tree}$$
</div>

where $$ \Delta $$ indicates a hamming loss, which is a commonly used to indicate loss in multi label classification task.
This makes sense because multiple labelings are conducted on a tree for each constituent - as VB, NP, ADJP, and etc.
I recommend refering to the paper for more details on the architecture as what drove me to discuss this paper is on the next section.


## Separating Content and Position Information

Kitaev and Klein claim the following: content and position information should be balanced throughout learning, but in practice it does not.
Thus, they seek for a way to separate the information within the network.

Given the equation $$ z_t = w_t + m_t + p_t $$, where $$ w_t, m_t, p_t $$ each means word, tag, and positional embedding,
approach using this was limitted in its peformance as positional information dominates content information.

Authors' first attempt was to separate information by concatanating as such: $$ z_t = [w_t + m_t;p_t] $$.
However, regarding this approach intermingles both information in high deimention network, this method results the same.
For a better explanation, given the following relation $$ q = q^{(c)} + q^{(p)} $$ and $$ k = k^{(c)} + k^{(p)} $$,
where $$c$$ and $$p$$ each means content and position information,
throughout attention mechanism, we see they intermingle and loss balance eventually.

<div style="width: 100%; overflow: scroll;">
    $$ q\cdot k = (q^{(c)} + q^{(p)})\cdot (k^{(c)} + k^{(p)}) $$
</div>

This happens because weights multiplied to key and value are the _weighted sum of both_ content and position information.
Thus, authors decide to completely _factor_ out each inforamtion by reforming weights operated on input like below,

<div style="width: 100%; overflow: scroll;">
    $$ W = \begin{bmatrix} W^{(c)} & 0 \\ 0 & W^{(p)} \end{bmatrix} $$
</div>

Then, $$ W c = [W^{(c)}x^{(c)};W^{(p)}x^{(p)}]$$, which means unlike their initial approach, 

<div style="width: 100%; overflow: scroll;">
    $$ q\cdot k = q^{(c)}\cdot k^{(c)} + q^{(p)}\cdot k^{(p)}) $$
</div>