---
title: Byte Pair Encoding in NLP
tags: ml nlp
img_url: /assets/img/bpe.png
layout: blog
---

This is a short summary of a paper, _Neural Machine Translation of Rare Words with Subwords Units_ by _Sennrich (2016)_.
The approach to formulate a dictionary of corpus is very similar to WordPiece Model proposed by Schuster (2012), which both resolves OOV (out-of-vocabulary) cases by being able to create infinite words with subwords. 

## Motivation
Before the dicussion of its algorithm, it will be better to tackle on why new approach of formulating vocabulary is desired.
Original works include having UNK vocabulary or back-off strategy to cope with OOV cases.
These issues were brought forth due to their fixed vocabulary size.
Using subwords, however, can avoid this because combinations of subwords can create infinite number of words, and to make subwords, we need to segment words into smaller pieces.

However, one question can follow: would it work better?
The answer to that question is simply, yes (in neural translation model specifically).
Languages, not limited only to English, contains such common cases: named entities, cognates and loanwords, morphologically complex words. And, these narrow down to two things to consider: _compounding_ which is to combine words for a more complex word (air + plane -> airplane) and _transliteration_ which is a translation from letter to letter (g, b -> ㄱ, ㅂ).

BPE is not a magic that solves any issues unless the balance is found. Size of vocabulary may be small (even down to alphabet level size), and it will still cover limitless words. This of course will be able to maximize time and space efficiency, but with the major cost of delievering information in the sequence.
In other words, small size words will result the same input to be a longer sequence than bigger size words, in which it becomes more dfficult to contain information from its vicinity. 

## Algorithm
BPE encoding is a compression algorithm that iteratively pairs with the most frequent pairs. In my opinion, this distinct itself from WPM because Schuster (2012) picks pair that optimize language model rather than sepcifically for pair's frequency.
The code is provided in the paper like the following:

<!-- ``` python -->
{% highlight python %}
import re, collections

def get_stats(vocab):
    pairs = collections.defaultdict(int)
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols)-1):
            pairs[symbols[i], symbols[i+1]] += freq
    return pairs

def merge_vocab(pair, v_in):
    v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
        w_out = p.sub(''.join(pair), word)
        v_out[w_out] = v_in[word]
    return v_out

vocab = {'l o w </w>': 5, 'l o w e r </w>': 2,
         'n e w e s t </w>': 6, 'w i d e s t </w>': 3}

num_merges = 10
for i in range(num_merges):
    pairs = get_stats(vocab)
    best = max(pairs, key=pairs.get)
    vocab = merge_vocab(best, vocab)
    print(best)
{% endhighlight %}
<!-- ``` -->

When the code is run, we get the ouput like the following.

<div style="text-align: center;">
('e', 's')<br>
('es', 't')<br>
<b>('est', '</w>')</b><br>
('l', 'o')<br>
('lo', 'w')<br>
('n', 'e')<br>
('ne', 'w')<br>
<b>('new', 'est</w>')</b><br>
('low', '</w>')<br>
('w', 'i')<br>
<br>
</div>
Note that 'est' which has occurred frequently later combine with 'new' and 'low' as a whole.
The role of comparison 'est' is cached and combination with other subwords shows a transparent transformation and later help transparent translation, meaning _"adjective + 'est'"_ is _"'가장' + 형용사"_ in Korean and parrallel translation can be found within.